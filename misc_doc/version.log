v0.6.5
1. [Fix] Bug in UniTensor _Load    
2. [Enhance] Improve stability in Lanczos_ER  
3. [Enhance] Move _SII to stack.
4. [Enhance] Add LinOp operator() for mv_elem
5. [Enhance] Add c++ API fast link to cutt
6. [Enhance] Add Fromfile/Tofile for load/save binary files @ Tensor/Storage
7. [Enhance] Add linspace generator
8. [Fix] Bug in Div for fast Blas call bug
9. [Enhance] Add Tensor.append(Storage) if Tensor is rank-2 and dimension match.
10. [Enhance] Add algo namespace
11. [Enhance] Add Sort-@cpu
12. [Enhance] add Storage.numpy() for pythonAPI
13. [Enhance] add Tensor.from_storage() for python API

v0.6.4
1. [Enhance] Add option mv_elem for Tensordot, which actually move elements in input tensor. This is beneficial when same tensordot is called multiple times.
2. [Enhance] Add option cacheL, cacheR to Contract of unitensor. which mv the elements of input tensors to the matmul handy position. 
3. [Enhance] optimize Network contraction policy to reduce contiguous permute, with is_clone argument when PutUniTensor.
4. [Enhance] Add Lanczos_Gnd for fast get ground state and it's eigen value (currently only real float). 
5. [Enhance] Add Tridiag python API, and option is_row
6. [Enhance] C++ API storage add .back<>() function. 
7. [Enhance] C++ API storage fix from_vector() for bool type. 
8. [Enhance] Change Network Launch optimal=True behavior. if user order is given, optimal will not have effect.   
9. [Enhance] Add example/iDMRG/dmrg_optim.py for better performace with Lanczos_Gnd and Network cache.
10. [Fix] wrong error message in linalg::Cpr
11. [Fix] reshape() on a already contiguous Tensor will resulting as the change in original tensor, which should not happened.

v0.6.3
1. [Enhance] Add Device.Ncpus for detecting avaliable omp threads
2. [Enhance] Add HPTT support on CPU permute.
3. [Internal] Build version centralize
4. [Enhance] More info for Device. 
6. [Enhance] Add cytnx.__variant_info__ for checking the installed variant.

v0.6.2
1. [Fix] Bug in CUDA Matmul interface passing the wrong object bug.  
2. [Enhance] Add Matmul_dg for diagonal matrix mutiply dense matrix. 
3. [Enhance] Add Tensordot_dg for tensordot with either Tl or Tr is diagonal matrix
4. [Enhance] Contract dense & sparse memory optimized.      
5. [example] Add iTEBD_gpu.py example
6. [Fix] Bug in CUDA cuVectordot d and f seg fault 
7. [Enhance] Add cuReduce for optimized reduction. 
8. [Enhance] Optimize performance for Mul_internal_cpu. 
9. [Enhance] Optimize performance for Div_internal_cpu. 
10. [Fix] Bug in permute of UniTensor/Tensor with duplicate entries does not return error.

v0.6.1
1. [Enhance] add Scalar class (shadow)
2. [Enhance] change default allocation from Malloc to Calloc.
3. [Enhance] change storage.raw_ptr() to storage.data() and storage.data<>() 
4. [Enhance] change storage.cap to STORAGE_DEFT_SZ that can be tune.
5. [Enhance] adding Tproxy/Tproxy, Tproxy/Tensor, Tensor/Tproxy operation 
6. [Enhance] Add mv_elem type for LinOp, which intrinsically omp the matvec operation.    
7. [Fatal  ] Fix bug in Dot for Matrix-Vector multiplication on both GPU and CPU with complex&real float dtypes.

v0.6.0
1. [Enhance] behavior change the behavior of permute to prevent redundant copy in UniTensor and Tensor.
2. add Tensor::same_data to check if two Tensor has same storage.
3. [Enhance] the behavior of reshape in Tensor to prevent redundant copy.    
4. [Enhance] behavior change all linalg to follow the same disipline for permute/reshape/contiguous
5. [Enhance] add print() in C++ API    
6. [Fix] reshape() does not share memory
7. [Fix] BoolStorage print_elem does not show the first element in shape


v0.5.6a
1. [Enhance] change linalg::QR -> linalg::Qr for unify the function call 
2. Fix bug in UniTensor Qr, R UniTensor labels bug.
3. Add Qdr for UniTensor and Tensor.
4. Fix minor bug in internal, Type.is_float for Uint32.
5. [Enhance] accessor can now specify with vector. 
6. [Enhance] Tproxy.item()
7. Fix inplace reshape_() in new way templ. does not perform inplace operation
8. [Enhance] Tproxy operator+-/*
10. Fix bug in division dti64 becomes subtraction bug. 

v0.5.5a
1. [Feature] Tensor can now using operator() to access elements just like python. 
2. [Enhance] Access Tensor can now exactly the same using slice string as in python.
3. [Enhance] at/reshape/permute in Tensor can now give args without braket{} as in python.
4. [Enhance] Storage.Load to static, so it can match Tensor
5. [Major] Remove cytnx_extension class, rename CyTensor->UniTensor 
6. Fix small bug in return ref of Tproxy 
7. Fix bug in buffer size allocation in Svd_internal 
   

v0.5.4a-build1
1. [Important] Fix Subtraction real - complex bug.

v0.5.4a
1. Add linalg::Det 
2. Add Type.is_float
3. [Feature] Add LinOp class for custom linear operators used in iterative solver
4. enhance arithmetic with scalar Tensors
5. Add Tensor append with tensor. 
6. [Feature] Add iterative solver Lanczos_ER
7. [Enhance] Tproxy +=,-=,/=,*= on C++ side
8. Add ED (using Lanczos) example.
9. Change backend to mkl_ilp64, w/o mkl: OpenBLAS
10. Change Rowrank->rowrank for CyTensor. 

v0.5.3a
1. Add xlinalg.QR
2. enhance hosvd.
3. Fix bug in cytnx.linalg.Abs truncate the floating point part. 
4. Add example for HOTRG
5. Add example for iDMRG
6. Add CyTensor.truncate/truncate.
7. Add linalg::Sum. 
8. Complete set_elem for sparse CyTensor dispatch in binding.
9. [Important] Change Inv/Inv_ to InvM/InvM_ for matrix inverse. 
10. [Important] Add Inv/Inv_ for elementwise inverse with clip. 
11. [Enhance] Add str_strip for removing " ", "\t", "\r" at the end.
12. [Enhance] Accessor::() allow negative input.
13. Add GPU Pow/Pow_
14. Add random.uniform()
15. Fix bug in diagonal CyTensor reshape/reshape_ cause mismatch.
16. Add a is_diag option for convert Tensor to CyTensor. 


v0.5.2a-build1
1. example/iTEBD, please modify the argument rowrank->Rowrank if you encounter error in running them.
2. Fix bug in cytnx.linalg.Abs truncate floating point part. ---> v0.5.2a-build1
3. Fix bug in mkl blas package import bug with numpy.        ---> v0.5.2a-build1


v0.5.2a
1. add Trace and Trace_ for CyTensor.
2. fix bug in Network.Launch does not return the output CyTensor
3. Add Network.PrintNet, and ostream support.
4. Add Network.Diagram() for plot the tensor network diagram (python only)    
5. Add support for floating type Vectordot on GPU. 
6. Fix bug in to from Anytype to ComplexFloat. 
7. Add QR for CPU.
8. Add identity() and it's alias function eye(). 
9. Add physics namespace/submodule
10. Add physics::spin() for generating Spin-S representation. 
11. Add physics::pauli() for pauli matrix.
12. Add ExpM() for generic matrix (CPU only)
13. Fix bug in python slice, and reverse range slice.
14. Enhance optional Kron padding scheme
15. Fix bug in CyTensor contract/Contract(A,B) for tensors with no common label
16. Enhance error message in Network
17. Add Min(), Max() (CPU only)
18. Fix bug in Abs. 
19. Fix bug in Div i32td.    
20. [Feature] Add optimal contraction order calculation in Network
21. Fix SparseCyTensor contiguous address wrong calculation. 
22. Support at() directly from SparseCyTensor.
23. Add Transpose, Dagger to CyTensor. For tagged CyTensor, Transpose/Dagger will reverse the direction of all bonds.
24. Add xlinalg.Svd, xlinalg.Svd_truncate support for tagged CyTensor.
25. Fix redundant print in optimal contraction order
26. Add CyTensor.tag() for DenseCyTensor (regular type) directly convert to CyTensor with direction (tagged type)
27. Add SparseCyTensor.at (currently only floating point type) 
28. SparseCyTensor.ele_exists. 
29. SparseCyTensor.Transpose, Conj. 
30. Symmetry.reverse_rule, Bond.calc_reverse_qnums
31. Fix Tensor.numpy from GPU bug.
32. Fix Tensor.setitem/getitem pybind bug.
33. SparseCyTensor.get_elem/set_elem (currently floating type only (complex)) 
34. Add xlinalg::ExpH, xlinalg::ExpM, xlinalg::Trace (ovld of CyTensor.Trace)
35. support Mul/Div operation on SparseCyTensor 
36. Add Tensor.flatten();
37. Add Network.Savefile. Network.PutCyTensors
38. [Feature] Tensor can now use unify operator[] to get and set elements as python API
39. fix ambiguous error message in Tensor arithmetic.
40. fix bug in xlinalg::Svd
41. fix bug in physics::pauli
42. fix bug in CyTensor.set_label checking element.
43. Add xlinalg::Hosvd (currently CyTensor only)
44. change argument of init CyTensor rowrank->Rowrank
45. Add PESS example 
46. Add support for Norm to generic rank-N Tensor
47. Add @ operator in python API for shorthand of linalg::Dot
48. Add DMRG example
49. C++ API can now have accessor.size() < rank()
50. Remove redundant output of Inv.
51. Add Pow, Pow_ for CyTensor.
52. Add Symmetry.Save/Load
53. Symmetry/Tensor/Storage/Bond/CyTensor Save/Load re-invented for more simple usage


v0.5.1a
1. add Norm() for CPU and GPU, add to call by Tn
2. add Dot() for CPU and GPU, with unify API for Vec-Vec/Mat-Vec/Mat-Mat/Ten-Vec product.
3. add Tensor.rank() 
4. [Feature] support Tensor <-> numpy.ndarray
5. add random::Make_uniform()
6. Fix bug in Svd_truncate that will change the underlying block for contiguous CyTensor. 
7. Fix bug in Tensor->numpy if the underlying Tensor is non-contiguous. 
8. Add Eig.
9. Add real() imag() for Tensor. 
10. Enhance python API, Storage & Tensor are now iterable.
11. Fix buf in Conj and Conj_, for both C++ and python
12. Fix bug python inplace call return ID Conj_, Inv_, Exp_
13. Add Conj, Conj_ for CyTensor
14. Fix non-inplace Arithmetic for non-contiguous tensor. 
15. Add [trial version] Trace. 
16. Add Pow, Pow_ for cpu. 
17. Add Abs, Abs_ for cpu.

v0.5.0a
1. Add .imag() .real() for Storage. 
2. Add xlinalg under cytnx_extension for algebra on CyTensor
3. Add xlinalg::Svd()  
4. Change linalg::Eigh() to match numpy 
5. fix Diag uninitialize elemets bug
6. add linalg::ExpH()
7. add random::Make_normal()
8. add iTEBD example for both C++ and python @ example/iTEBD

v0.4
1. remove Otimes, add Kron and Outer 
2. Add Storage append, capacity, pre-alloc 32x address
3. Tensor can now allow redundant dimension (e.g. shape = (1,1,1,1,1...) 
4. Add Storage.from_vector, directly convert the C++ vector to Storage
5. Add more intruisive way to get slices for Tensor in C++, using operator[]
6. Add Tensor.append for rank-1 Tensor    
7. Add Exp() Expf() Exp\_() Expf\_()
8. Change UniTensor to CyTensor 
9. Guarded CyTensor, Bond, Symmetry and Network class with cytnx_extension namespace (cytnx_extension submodule in python).  
