<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.5" xml:lang="en-US">
  <compounddef id="version_log" kind="page">
    <compoundname>version_log</compoundname>
    <title>Version log</title>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<para><programlisting filename="misc_doc/version.log"><codeline><highlight class="normal">v0.6.5</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Fix]<sp/>Bug<sp/>in<sp/>UniTensor<sp/>_Load<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>[Enhance]<sp/>Improve<sp/>stability<sp/>in<sp/>Lanczos_ER<sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Enhance]<sp/>Move<sp/>_SII<sp/>to<sp/>stack.</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>Add<sp/>LinOp<sp/>operator()<sp/>for<sp/>mv_elem</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[Enhance]<sp/>Add<sp/>c++<sp/>API<sp/>fast<sp/>link<sp/>to<sp/>cutt</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Enhance]<sp/>Add<sp/>Fromfile/Tofile<sp/>for<sp/>load/save<sp/>binary<sp/>files<sp/>@<sp/>Tensor/Storage</highlight></codeline>
<codeline><highlight class="normal">7.<sp/>[Enhance]<sp/>Add<sp/>linspace<sp/>generator</highlight></codeline>
<codeline><highlight class="normal">8.<sp/>[Fix]<sp/>Bug<sp/>in<sp/>Div<sp/>for<sp/>fast<sp/>Blas<sp/>call<sp/>bug</highlight></codeline>
<codeline><highlight class="normal">9.<sp/>[Enhance]<sp/>Add<sp/>Tensor.append(Storage)<sp/>if<sp/>Tensor<sp/>is<sp/>rank-2<sp/>and<sp/>dimension<sp/>match.</highlight></codeline>
<codeline><highlight class="normal">10.<sp/>[Enhance]<sp/>Add<sp/>algo<sp/>namespace</highlight></codeline>
<codeline><highlight class="normal">11.<sp/>[Enhance]<sp/>Add<sp/>Sort-@cpu</highlight></codeline>
<codeline><highlight class="normal">12.<sp/>[Enhance]<sp/>add<sp/>Storage.numpy()<sp/>for<sp/>pythonAPI</highlight></codeline>
<codeline><highlight class="normal">13.<sp/>[Enhance]<sp/>add<sp/>Tensor.from_storage()<sp/>for<sp/>python<sp/>API</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.6.4</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Enhance]<sp/>Add<sp/>option<sp/>mv_elem<sp/>for<sp/>Tensordot,<sp/>which<sp/>actually<sp/>move<sp/>elements<sp/>in<sp/>input<sp/>tensor.<sp/>This<sp/>is<sp/>beneficial<sp/>when<sp/>same<sp/>tensordot<sp/>is<sp/>called<sp/>multiple<sp/>times.</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>[Enhance]<sp/>Add<sp/>option<sp/>cacheL,<sp/>cacheR<sp/>to<sp/>Contract<sp/>of<sp/>unitensor.<sp/>which<sp/>mv<sp/>the<sp/>elements<sp/>of<sp/>input<sp/>tensors<sp/>to<sp/>the<sp/>matmul<sp/>handy<sp/>position.<sp/></highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Enhance]<sp/>optimize<sp/>Network<sp/>contraction<sp/>policy<sp/>to<sp/>reduce<sp/>contiguous<sp/>permute,<sp/>with<sp/>is_clone<sp/>argument<sp/>when<sp/>PutUniTensor.</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>Add<sp/>Lanczos_Gnd<sp/>for<sp/>fast<sp/>get<sp/>ground<sp/>state<sp/>and<sp/>it&apos;s<sp/>eigen<sp/>value<sp/>(currently<sp/>only<sp/>real<sp/>float).<sp/></highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[Enhance]<sp/>Add<sp/>Tridiag<sp/>python<sp/>API,<sp/>and<sp/>option<sp/>is_row</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Enhance]<sp/>C++<sp/>API<sp/>storage<sp/>add<sp/>.back&lt;&gt;()<sp/>function.<sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>[Enhance]<sp/>C++<sp/>API<sp/>storage<sp/>fix<sp/>from_vector()<sp/>for<sp/>bool<sp/>type.<sp/></highlight></codeline>
<codeline><highlight class="normal">8.<sp/>[Enhance]<sp/>Change<sp/>Network<sp/>Launch<sp/>optimal=True<sp/>behavior.<sp/>if<sp/>user<sp/>order<sp/>is<sp/>given,<sp/>optimal<sp/>will<sp/>not<sp/>have<sp/>effect.<sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">9.<sp/>[Enhance]<sp/>Add<sp/>example/iDMRG/dmrg_optim.py<sp/>for<sp/>better<sp/>performace<sp/>with<sp/>Lanczos_Gnd<sp/>and<sp/>Network<sp/>cache.</highlight></codeline>
<codeline><highlight class="normal">10.<sp/>[Fix]<sp/>wrong<sp/>error<sp/>message<sp/>in<sp/>linalg::Cpr</highlight></codeline>
<codeline><highlight class="normal">11.<sp/>[Fix]<sp/>reshape()<sp/>on<sp/>a<sp/>already<sp/>contiguous<sp/>Tensor<sp/>will<sp/>resulting<sp/>as<sp/>the<sp/>change<sp/>in<sp/>original<sp/>tensor,<sp/>which<sp/>should<sp/>not<sp/>happened.</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.6.3</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Enhance]<sp/>Add<sp/>Device.Ncpus<sp/>for<sp/>detecting<sp/>avaliable<sp/>omp<sp/>threads</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>[Enhance]<sp/>Add<sp/>HPTT<sp/>support<sp/>on<sp/>CPU<sp/>permute.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Internal]<sp/>Build<sp/>version<sp/>centralize</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>More<sp/>info<sp/>for<sp/>Device.<sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Enhance]<sp/>Add<sp/>cytnx.__variant_info__<sp/>for<sp/>checking<sp/>the<sp/>installed<sp/>variant.</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.6.2</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Fix]<sp/>Bug<sp/>in<sp/>CUDA<sp/>Matmul<sp/>interface<sp/>passing<sp/>the<sp/>wrong<sp/>object<sp/>bug.<sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>[Enhance]<sp/>Add<sp/>Matmul_dg<sp/>for<sp/>diagonal<sp/>matrix<sp/>mutiply<sp/>dense<sp/>matrix.<sp/></highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Enhance]<sp/>Add<sp/>Tensordot_dg<sp/>for<sp/>tensordot<sp/>with<sp/>either<sp/>Tl<sp/>or<sp/>Tr<sp/>is<sp/>diagonal<sp/>matrix</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>Contract<sp/>dense<sp/>&amp;<sp/>sparse<sp/>memory<sp/>optimized.<sp/><sp/><sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[example]<sp/>Add<sp/>iTEBD_gpu.py<sp/>example</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Fix]<sp/>Bug<sp/>in<sp/>CUDA<sp/>cuVectordot<sp/>d<sp/>and<sp/>f<sp/>seg<sp/>fault<sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>[Enhance]<sp/>Add<sp/>cuReduce<sp/>for<sp/>optimized<sp/>reduction.<sp/></highlight></codeline>
<codeline><highlight class="normal">8.<sp/>[Enhance]<sp/>Optimize<sp/>performance<sp/>for<sp/>Mul_internal_cpu.<sp/></highlight></codeline>
<codeline><highlight class="normal">9.<sp/>[Enhance]<sp/>Optimize<sp/>performance<sp/>for<sp/>Div_internal_cpu.<sp/></highlight></codeline>
<codeline><highlight class="normal">10.<sp/>[Fix]<sp/>Bug<sp/>in<sp/>permute<sp/>of<sp/>UniTensor/Tensor<sp/>with<sp/>duplicate<sp/>entries<sp/>does<sp/>not<sp/>return<sp/>error.</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.6.1</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Enhance]<sp/>add<sp/>Scalar<sp/>class<sp/>(shadow)</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>[Enhance]<sp/>change<sp/>default<sp/>allocation<sp/>from<sp/>Malloc<sp/>to<sp/>Calloc.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Enhance]<sp/>change<sp/>storage.raw_ptr()<sp/>to<sp/>storage.data()<sp/>and<sp/>storage.data&lt;&gt;()<sp/></highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>change<sp/>storage.cap<sp/>to<sp/>STORAGE_DEFT_SZ<sp/>that<sp/>can<sp/>be<sp/>tune.</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[Enhance]<sp/>adding<sp/>Tproxy/Tproxy,<sp/>Tproxy/Tensor,<sp/>Tensor/Tproxy<sp/>operation<sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Enhance]<sp/>Add<sp/>mv_elem<sp/>type<sp/>for<sp/>LinOp,<sp/>which<sp/>intrinsically<sp/>omp<sp/>the<sp/>matvec<sp/>operation.<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>[Fatal<sp/><sp/>]<sp/>Fix<sp/>bug<sp/>in<sp/>Dot<sp/>for<sp/>Matrix-Vector<sp/>multiplication<sp/>on<sp/>both<sp/>GPU<sp/>and<sp/>CPU<sp/>with<sp/>complex&amp;real<sp/>float<sp/>dtypes.</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.6.0</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Enhance]<sp/>behavior<sp/>change<sp/>the<sp/>behavior<sp/>of<sp/>permute<sp/>to<sp/>prevent<sp/>redundant<sp/>copy<sp/>in<sp/>UniTensor<sp/>and<sp/>Tensor.</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>add<sp/>Tensor::same_data<sp/>to<sp/>check<sp/>if<sp/>two<sp/>Tensor<sp/>has<sp/>same<sp/>storage.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Enhance]<sp/>the<sp/>behavior<sp/>of<sp/>reshape<sp/>in<sp/>Tensor<sp/>to<sp/>prevent<sp/>redundant<sp/>copy.<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>behavior<sp/>change<sp/>all<sp/>linalg<sp/>to<sp/>follow<sp/>the<sp/>same<sp/>disipline<sp/>for<sp/>permute/reshape/contiguous</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[Enhance]<sp/>add<sp/>print()<sp/>in<sp/>C++<sp/>API<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Fix]<sp/>reshape()<sp/>does<sp/>not<sp/>share<sp/>memory</highlight></codeline>
<codeline><highlight class="normal">7.<sp/>[Fix]<sp/>BoolStorage<sp/>print_elem<sp/>does<sp/>not<sp/>show<sp/>the<sp/>first<sp/>element<sp/>in<sp/>shape</highlight></codeline>
<codeline></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.6a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Enhance]<sp/>change<sp/>linalg::QR<sp/>-&gt;<sp/>linalg::Qr<sp/>for<sp/>unify<sp/>the<sp/>function<sp/>call<sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>Fix<sp/>bug<sp/>in<sp/>UniTensor<sp/>Qr,<sp/>R<sp/>UniTensor<sp/>labels<sp/>bug.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>Add<sp/>Qdr<sp/>for<sp/>UniTensor<sp/>and<sp/>Tensor.</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>Fix<sp/>minor<sp/>bug<sp/>in<sp/>internal,<sp/>Type.is_float<sp/>for<sp/>Uint32.</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[Enhance]<sp/>accessor<sp/>can<sp/>now<sp/>specify<sp/>with<sp/>vector.<sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Enhance]<sp/>Tproxy.item()</highlight></codeline>
<codeline><highlight class="normal">7.<sp/>Fix<sp/>inplace<sp/>reshape_()<sp/>in<sp/>new<sp/>way<sp/>templ.<sp/>does<sp/>not<sp/>perform<sp/>inplace<sp/>operation</highlight></codeline>
<codeline><highlight class="normal">8.<sp/>[Enhance]<sp/>Tproxy<sp/>operator+-/*</highlight></codeline>
<codeline><highlight class="normal">10.<sp/>Fix<sp/>bug<sp/>in<sp/>division<sp/>dti64<sp/>becomes<sp/>subtraction<sp/>bug.<sp/></highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.5a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Feature]<sp/>Tensor<sp/>can<sp/>now<sp/>using<sp/>operator()<sp/>to<sp/>access<sp/>elements<sp/>just<sp/>like<sp/>python.<sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>[Enhance]<sp/>Access<sp/>Tensor<sp/>can<sp/>now<sp/>exactly<sp/>the<sp/>same<sp/>using<sp/>slice<sp/>string<sp/>as<sp/>in<sp/>python.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Enhance]<sp/>at/reshape/permute<sp/>in<sp/>Tensor<sp/>can<sp/>now<sp/>give<sp/>args<sp/>without<sp/>braket{}<sp/>as<sp/>in<sp/>python.</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Enhance]<sp/>Storage.Load<sp/>to<sp/>static,<sp/>so<sp/>it<sp/>can<sp/>match<sp/>Tensor</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>[Major]<sp/>Remove<sp/>cytnx_extension<sp/>class,<sp/>rename<sp/>CyTensor-&gt;UniTensor<sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>Fix<sp/>small<sp/>bug<sp/>in<sp/>return<sp/>ref<sp/>of<sp/>Tproxy<sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>Fix<sp/>bug<sp/>in<sp/>buffer<sp/>size<sp/>allocation<sp/>in<sp/>Svd_internal<sp/></highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/></highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.4a-build1</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>[Important]<sp/>Fix<sp/>Subtraction<sp/>real<sp/>-<sp/>complex<sp/>bug.</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.4a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>Add<sp/>linalg::Det<sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>Add<sp/>Type.is_float</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>[Feature]<sp/>Add<sp/>LinOp<sp/>class<sp/>for<sp/>custom<sp/>linear<sp/>operators<sp/>used<sp/>in<sp/>iterative<sp/>solver</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>enhance<sp/>arithmetic<sp/>with<sp/>scalar<sp/>Tensors</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>Add<sp/>Tensor<sp/>append<sp/>with<sp/>tensor.<sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>[Feature]<sp/>Add<sp/>iterative<sp/>solver<sp/>Lanczos_ER</highlight></codeline>
<codeline><highlight class="normal">7.<sp/>[Enhance]<sp/>Tproxy<sp/>+=,-=,/=,*=<sp/>on<sp/>C++<sp/>side</highlight></codeline>
<codeline><highlight class="normal">8.<sp/>Add<sp/>ED<sp/>(using<sp/>Lanczos)<sp/>example.</highlight></codeline>
<codeline><highlight class="normal">9.<sp/>Change<sp/>backend<sp/>to<sp/>mkl_ilp64,<sp/>w/o<sp/>mkl:<sp/>OpenBLAS</highlight></codeline>
<codeline><highlight class="normal">10.<sp/>Change<sp/>Rowrank-&gt;rowrank<sp/>for<sp/>CyTensor.<sp/></highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.3a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>Add<sp/>xlinalg.QR</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>enhance<sp/>hosvd.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>Fix<sp/>bug<sp/>in<sp/>cytnx.linalg.Abs<sp/>truncate<sp/>the<sp/>floating<sp/>point<sp/>part.<sp/></highlight></codeline>
<codeline><highlight class="normal">4.<sp/>Add<sp/>example<sp/>for<sp/>HOTRG</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>Add<sp/>example<sp/>for<sp/>iDMRG</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>Add<sp/>CyTensor.truncate/truncate.</highlight></codeline>
<codeline><highlight class="normal">7.<sp/>Add<sp/>linalg::Sum.<sp/></highlight></codeline>
<codeline><highlight class="normal">8.<sp/>Complete<sp/>set_elem<sp/>for<sp/>sparse<sp/>CyTensor<sp/>dispatch<sp/>in<sp/>binding.</highlight></codeline>
<codeline><highlight class="normal">9.<sp/>[Important]<sp/>Change<sp/>Inv/Inv_<sp/>to<sp/>InvM/InvM_<sp/>for<sp/>matrix<sp/>inverse.<sp/></highlight></codeline>
<codeline><highlight class="normal">10.<sp/>[Important]<sp/>Add<sp/>Inv/Inv_<sp/>for<sp/>elementwise<sp/>inverse<sp/>with<sp/>clip.<sp/></highlight></codeline>
<codeline><highlight class="normal">11.<sp/>[Enhance]<sp/>Add<sp/>str_strip<sp/>for<sp/>removing<sp/>&quot;<sp/>&quot;,<sp/>&quot;\t&quot;,<sp/>&quot;\r&quot;<sp/>at<sp/>the<sp/>end.</highlight></codeline>
<codeline><highlight class="normal">12.<sp/>[Enhance]<sp/>Accessor::()<sp/>allow<sp/>negative<sp/>input.</highlight></codeline>
<codeline><highlight class="normal">13.<sp/>Add<sp/>GPU<sp/>Pow/Pow_</highlight></codeline>
<codeline><highlight class="normal">14.<sp/>Add<sp/>random.uniform()</highlight></codeline>
<codeline><highlight class="normal">15.<sp/>Fix<sp/>bug<sp/>in<sp/>diagonal<sp/>CyTensor<sp/>reshape/reshape_<sp/>cause<sp/>mismatch.</highlight></codeline>
<codeline><highlight class="normal">16.<sp/>Add<sp/>a<sp/>is_diag<sp/>option<sp/>for<sp/>convert<sp/>Tensor<sp/>to<sp/>CyTensor.<sp/></highlight></codeline>
<codeline></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.2a-build1</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>example/iTEBD,<sp/>please<sp/>modify<sp/>the<sp/>argument<sp/>rowrank-&gt;Rowrank<sp/>if<sp/>you<sp/>encounter<sp/>error<sp/>in<sp/>running<sp/>them.</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>Fix<sp/>bug<sp/>in<sp/>cytnx.linalg.Abs<sp/>truncate<sp/>floating<sp/>point<sp/>part.<sp/>---&gt;<sp/>v0.5.2a-build1</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>Fix<sp/>bug<sp/>in<sp/>mkl<sp/>blas<sp/>package<sp/>import<sp/>bug<sp/>with<sp/>numpy.<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>---&gt;<sp/>v0.5.2a-build1</highlight></codeline>
<codeline></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.2a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>add<sp/>Trace<sp/>and<sp/>Trace_<sp/>for<sp/>CyTensor.</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>fix<sp/>bug<sp/>in<sp/>Network.Launch<sp/>does<sp/>not<sp/>return<sp/>the<sp/>output<sp/>CyTensor</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>Add<sp/>Network.PrintNet,<sp/>and<sp/>ostream<sp/>support.</highlight></codeline>
<codeline><highlight class="normal">4.<sp/>Add<sp/>Network.Diagram()<sp/>for<sp/>plot<sp/>the<sp/>tensor<sp/>network<sp/>diagram<sp/>(python<sp/>only)<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">5.<sp/>Add<sp/>support<sp/>for<sp/>floating<sp/>type<sp/>Vectordot<sp/>on<sp/>GPU.<sp/></highlight></codeline>
<codeline><highlight class="normal">6.<sp/>Fix<sp/>bug<sp/>in<sp/>to<sp/>from<sp/>Anytype<sp/>to<sp/>ComplexFloat.<sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>Add<sp/>QR<sp/>for<sp/>CPU.</highlight></codeline>
<codeline><highlight class="normal">8.<sp/>Add<sp/>identity()<sp/>and<sp/>it&apos;s<sp/>alias<sp/>function<sp/>eye().<sp/></highlight></codeline>
<codeline><highlight class="normal">9.<sp/>Add<sp/>physics<sp/>namespace/submodule</highlight></codeline>
<codeline><highlight class="normal">10.<sp/>Add<sp/>physics::spin()<sp/>for<sp/>generating<sp/>Spin-S<sp/>representation.<sp/></highlight></codeline>
<codeline><highlight class="normal">11.<sp/>Add<sp/>physics::pauli()<sp/>for<sp/>pauli<sp/>matrix.</highlight></codeline>
<codeline><highlight class="normal">12.<sp/>Add<sp/>ExpM()<sp/>for<sp/>generic<sp/>matrix<sp/>(CPU<sp/>only)</highlight></codeline>
<codeline><highlight class="normal">13.<sp/>Fix<sp/>bug<sp/>in<sp/>python<sp/>slice,<sp/>and<sp/>reverse<sp/>range<sp/>slice.</highlight></codeline>
<codeline><highlight class="normal">14.<sp/>Enhance<sp/>optional<sp/>Kron<sp/>padding<sp/>scheme</highlight></codeline>
<codeline><highlight class="normal">15.<sp/>Fix<sp/>bug<sp/>in<sp/>CyTensor<sp/>contract/Contract(A,B)<sp/>for<sp/>tensors<sp/>with<sp/>no<sp/>common<sp/>label</highlight></codeline>
<codeline><highlight class="normal">16.<sp/>Enhance<sp/>error<sp/>message<sp/>in<sp/>Network</highlight></codeline>
<codeline><highlight class="normal">17.<sp/>Add<sp/>Min(),<sp/>Max()<sp/>(CPU<sp/>only)</highlight></codeline>
<codeline><highlight class="normal">18.<sp/>Fix<sp/>bug<sp/>in<sp/>Abs.<sp/></highlight></codeline>
<codeline><highlight class="normal">19.<sp/>Fix<sp/>bug<sp/>in<sp/>Div<sp/>i32td.<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">20.<sp/>[Feature]<sp/>Add<sp/>optimal<sp/>contraction<sp/>order<sp/>calculation<sp/>in<sp/>Network</highlight></codeline>
<codeline><highlight class="normal">21.<sp/>Fix<sp/>SparseCyTensor<sp/>contiguous<sp/>address<sp/>wrong<sp/>calculation.<sp/></highlight></codeline>
<codeline><highlight class="normal">22.<sp/>Support<sp/>at()<sp/>directly<sp/>from<sp/>SparseCyTensor.</highlight></codeline>
<codeline><highlight class="normal">23.<sp/>Add<sp/>Transpose,<sp/>Dagger<sp/>to<sp/>CyTensor.<sp/>For<sp/>tagged<sp/>CyTensor,<sp/>Transpose/Dagger<sp/>will<sp/>reverse<sp/>the<sp/>direction<sp/>of<sp/>all<sp/>bonds.</highlight></codeline>
<codeline><highlight class="normal">24.<sp/>Add<sp/>xlinalg.Svd,<sp/>xlinalg.Svd_truncate<sp/>support<sp/>for<sp/>tagged<sp/>CyTensor.</highlight></codeline>
<codeline><highlight class="normal">25.<sp/>Fix<sp/>redundant<sp/>print<sp/>in<sp/>optimal<sp/>contraction<sp/>order</highlight></codeline>
<codeline><highlight class="normal">26.<sp/>Add<sp/>CyTensor.tag()<sp/>for<sp/>DenseCyTensor<sp/>(regular<sp/>type)<sp/>directly<sp/>convert<sp/>to<sp/>CyTensor<sp/>with<sp/>direction<sp/>(tagged<sp/>type)</highlight></codeline>
<codeline><highlight class="normal">27.<sp/>Add<sp/>SparseCyTensor.at<sp/>(currently<sp/>only<sp/>floating<sp/>point<sp/>type)<sp/></highlight></codeline>
<codeline><highlight class="normal">28.<sp/>SparseCyTensor.ele_exists.<sp/></highlight></codeline>
<codeline><highlight class="normal">29.<sp/>SparseCyTensor.Transpose,<sp/>Conj.<sp/></highlight></codeline>
<codeline><highlight class="normal">30.<sp/>Symmetry.reverse_rule,<sp/>Bond.calc_reverse_qnums</highlight></codeline>
<codeline><highlight class="normal">31.<sp/>Fix<sp/>Tensor.numpy<sp/>from<sp/>GPU<sp/>bug.</highlight></codeline>
<codeline><highlight class="normal">32.<sp/>Fix<sp/>Tensor.setitem/getitem<sp/>pybind<sp/>bug.</highlight></codeline>
<codeline><highlight class="normal">33.<sp/>SparseCyTensor.get_elem/set_elem<sp/>(currently<sp/>floating<sp/>type<sp/>only<sp/>(complex))<sp/></highlight></codeline>
<codeline><highlight class="normal">34.<sp/>Add<sp/>xlinalg::ExpH,<sp/>xlinalg::ExpM,<sp/>xlinalg::Trace<sp/>(ovld<sp/>of<sp/>CyTensor.Trace)</highlight></codeline>
<codeline><highlight class="normal">35.<sp/>support<sp/>Mul/Div<sp/>operation<sp/>on<sp/>SparseCyTensor<sp/></highlight></codeline>
<codeline><highlight class="normal">36.<sp/>Add<sp/>Tensor.flatten();</highlight></codeline>
<codeline><highlight class="normal">37.<sp/>Add<sp/>Network.Savefile.<sp/>Network.PutCyTensors</highlight></codeline>
<codeline><highlight class="normal">38.<sp/>[Feature]<sp/>Tensor<sp/>can<sp/>now<sp/>use<sp/>unify<sp/>operator[]<sp/>to<sp/>get<sp/>and<sp/>set<sp/>elements<sp/>as<sp/>python<sp/>API</highlight></codeline>
<codeline><highlight class="normal">39.<sp/>fix<sp/>ambiguous<sp/>error<sp/>message<sp/>in<sp/>Tensor<sp/>arithmetic.</highlight></codeline>
<codeline><highlight class="normal">40.<sp/>fix<sp/>bug<sp/>in<sp/>xlinalg::Svd</highlight></codeline>
<codeline><highlight class="normal">41.<sp/>fix<sp/>bug<sp/>in<sp/>physics::pauli</highlight></codeline>
<codeline><highlight class="normal">42.<sp/>fix<sp/>bug<sp/>in<sp/>CyTensor.set_label<sp/>checking<sp/>element.</highlight></codeline>
<codeline><highlight class="normal">43.<sp/>Add<sp/>xlinalg::Hosvd<sp/>(currently<sp/>CyTensor<sp/>only)</highlight></codeline>
<codeline><highlight class="normal">44.<sp/>change<sp/>argument<sp/>of<sp/>init<sp/>CyTensor<sp/>rowrank-&gt;Rowrank</highlight></codeline>
<codeline><highlight class="normal">45.<sp/>Add<sp/>PESS<sp/>example<sp/></highlight></codeline>
<codeline><highlight class="normal">46.<sp/>Add<sp/>support<sp/>for<sp/>Norm<sp/>to<sp/>generic<sp/>rank-N<sp/>Tensor</highlight></codeline>
<codeline><highlight class="normal">47.<sp/>Add<sp/>@<sp/>operator<sp/>in<sp/>python<sp/>API<sp/>for<sp/>shorthand<sp/>of<sp/>linalg::Dot</highlight></codeline>
<codeline><highlight class="normal">48.<sp/>Add<sp/>DMRG<sp/>example</highlight></codeline>
<codeline><highlight class="normal">49.<sp/>C++<sp/>API<sp/>can<sp/>now<sp/>have<sp/>accessor.size()<sp/>&lt;<sp/>rank()</highlight></codeline>
<codeline><highlight class="normal">50.<sp/>Remove<sp/>redundant<sp/>output<sp/>of<sp/>Inv.</highlight></codeline>
<codeline><highlight class="normal">51.<sp/>Add<sp/>Pow,<sp/>Pow_<sp/>for<sp/>CyTensor.</highlight></codeline>
<codeline><highlight class="normal">52.<sp/>Add<sp/>Symmetry.Save/Load</highlight></codeline>
<codeline><highlight class="normal">53.<sp/>Symmetry/Tensor/Storage/Bond/CyTensor<sp/>Save/Load<sp/>re-invented<sp/>for<sp/>more<sp/>simple<sp/>usage</highlight></codeline>
<codeline></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.1a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>add<sp/>Norm()<sp/>for<sp/>CPU<sp/>and<sp/>GPU,<sp/>add<sp/>to<sp/>call<sp/>by<sp/>Tn</highlight></codeline>
<codeline><highlight class="normal">2.<sp/>add<sp/>Dot()<sp/>for<sp/>CPU<sp/>and<sp/>GPU,<sp/>with<sp/>unify<sp/>API<sp/>for<sp/>Vec-Vec/Mat-Vec/Mat-Mat/Ten-Vec<sp/>product.</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>add<sp/>Tensor.rank()<sp/></highlight></codeline>
<codeline><highlight class="normal">4.<sp/>[Feature]<sp/>support<sp/>Tensor<sp/>&lt;-&gt;<sp/>numpy.ndarray</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>add<sp/>random::Make_uniform()</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>Fix<sp/>bug<sp/>in<sp/>Svd_truncate<sp/>that<sp/>will<sp/>change<sp/>the<sp/>underlying<sp/>block<sp/>for<sp/>contiguous<sp/>CyTensor.<sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>Fix<sp/>bug<sp/>in<sp/>Tensor-&gt;numpy<sp/>if<sp/>the<sp/>underlying<sp/>Tensor<sp/>is<sp/>non-contiguous.<sp/></highlight></codeline>
<codeline><highlight class="normal">8.<sp/>Add<sp/>Eig.</highlight></codeline>
<codeline><highlight class="normal">9.<sp/>Add<sp/>real()<sp/>imag()<sp/>for<sp/>Tensor.<sp/></highlight></codeline>
<codeline><highlight class="normal">10.<sp/>Enhance<sp/>python<sp/>API,<sp/>Storage<sp/>&amp;<sp/>Tensor<sp/>are<sp/>now<sp/>iterable.</highlight></codeline>
<codeline><highlight class="normal">11.<sp/>Fix<sp/>buf<sp/>in<sp/>Conj<sp/>and<sp/>Conj_,<sp/>for<sp/>both<sp/>C++<sp/>and<sp/>python</highlight></codeline>
<codeline><highlight class="normal">12.<sp/>Fix<sp/>bug<sp/>python<sp/>inplace<sp/>call<sp/>return<sp/>ID<sp/>Conj_,<sp/>Inv_,<sp/>Exp_</highlight></codeline>
<codeline><highlight class="normal">13.<sp/>Add<sp/>Conj,<sp/>Conj_<sp/>for<sp/>CyTensor</highlight></codeline>
<codeline><highlight class="normal">14.<sp/>Fix<sp/>non-inplace<sp/>Arithmetic<sp/>for<sp/>non-contiguous<sp/>tensor.<sp/></highlight></codeline>
<codeline><highlight class="normal">15.<sp/>Add<sp/>[trial<sp/>version]<sp/>Trace.<sp/></highlight></codeline>
<codeline><highlight class="normal">16.<sp/>Add<sp/>Pow,<sp/>Pow_<sp/>for<sp/>cpu.<sp/></highlight></codeline>
<codeline><highlight class="normal">17.<sp/>Add<sp/>Abs,<sp/>Abs_<sp/>for<sp/>cpu.</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.5.0a</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>Add<sp/>.imag()<sp/>.real()<sp/>for<sp/>Storage.<sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>Add<sp/>xlinalg<sp/>under<sp/>cytnx_extension<sp/>for<sp/>algebra<sp/>on<sp/>CyTensor</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>Add<sp/>xlinalg::Svd()<sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">4.<sp/>Change<sp/>linalg::Eigh()<sp/>to<sp/>match<sp/>numpy<sp/></highlight></codeline>
<codeline><highlight class="normal">5.<sp/>fix<sp/>Diag<sp/>uninitialize<sp/>elemets<sp/>bug</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>add<sp/>linalg::ExpH()</highlight></codeline>
<codeline><highlight class="normal">7.<sp/>add<sp/>random::Make_normal()</highlight></codeline>
<codeline><highlight class="normal">8.<sp/>add<sp/>iTEBD<sp/>example<sp/>for<sp/>both<sp/>C++<sp/>and<sp/>python<sp/>@<sp/>example/iTEBD</highlight></codeline>
<codeline></codeline>
<codeline><highlight class="normal">v0.4</highlight></codeline>
<codeline><highlight class="normal">1.<sp/>remove<sp/>Otimes,<sp/>add<sp/>Kron<sp/>and<sp/>Outer<sp/></highlight></codeline>
<codeline><highlight class="normal">2.<sp/>Add<sp/>Storage<sp/>append,<sp/>capacity,<sp/>pre-alloc<sp/>32x<sp/>address</highlight></codeline>
<codeline><highlight class="normal">3.<sp/>Tensor<sp/>can<sp/>now<sp/>allow<sp/>redundant<sp/>dimension<sp/>(e.g.<sp/>shape<sp/>=<sp/>(1,1,1,1,1...)<sp/></highlight></codeline>
<codeline><highlight class="normal">4.<sp/>Add<sp/>Storage.from_vector,<sp/>directly<sp/>convert<sp/>the<sp/>C++<sp/>vector<sp/>to<sp/>Storage</highlight></codeline>
<codeline><highlight class="normal">5.<sp/>Add<sp/>more<sp/>intruisive<sp/>way<sp/>to<sp/>get<sp/>slices<sp/>for<sp/>Tensor<sp/>in<sp/>C++,<sp/>using<sp/>operator[]</highlight></codeline>
<codeline><highlight class="normal">6.<sp/>Add<sp/>Tensor.append<sp/>for<sp/>rank-1<sp/>Tensor<sp/><sp/><sp/><sp/></highlight></codeline>
<codeline><highlight class="normal">7.<sp/>Add<sp/>Exp()<sp/>Expf()<sp/>Exp\_()<sp/>Expf\_()</highlight></codeline>
<codeline><highlight class="normal">8.<sp/>Change<sp/>UniTensor<sp/>to<sp/>CyTensor<sp/></highlight></codeline>
<codeline><highlight class="normal">9.<sp/>Guarded<sp/>CyTensor,<sp/>Bond,<sp/>Symmetry<sp/>and<sp/>Network<sp/>class<sp/>with<sp/>cytnx_extension<sp/>namespace<sp/>(cytnx_extension<sp/>submodule<sp/>in<sp/>python).<sp/><sp/></highlight></codeline>
</programlisting> </para>
    </detaileddescription>
    <location file="misc_doc/version_log.dox"/>
  </compounddef>
</doxygen>
