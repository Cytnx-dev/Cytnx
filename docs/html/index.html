<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Cytnx: Cytnx</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="Icon_small.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Cytnx
   &#160;<span id="projectnumber">v0.4</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Cytnx </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Version:</h2>
<p>v0.4</p>
<h2>What's new:</h2>
<ol type="1">
<li>remove Otimes, add Kron and Outer</li>
<li>Add Storage append, capacity, pre-alloc 32x address</li>
<li>Tensor can now allow redundant dimension (e.g. shape = (1,1,1,1,1...)</li>
<li>Add Storage.from_vector, directly convert the C++ vector to Storage</li>
<li>Add more intruisive way to get slices for Tensor in C++, using operator[]</li>
<li>Add Tensor.append for rank-1 Tensor</li>
<li>Add Exp() Expf() Exp_() Expf_()</li>
</ol>
<h2>Feature:</h2>
<h3>Python x C++</h3>
<p>Benefit from both side. One can do simple prototype on python side and easy transfer to C++ with small effort!</p>
<div class="fragment"><div class="line"><span class="comment">// c++ version:</span></div><div class="line"><span class="preprocessor">#include &quot;cytnx.hpp&quot;</span></div><div class="line"><a class="code" href="classcytnx_1_1Tensor.html">cytnx::Tensor</a> A({3,4,5},cytnx::Type.Double,cytnx::Device.cpu)</div></div><!-- fragment --><div class="fragment"><div class="line"><span class="comment"># python version:</span></div><div class="line"><span class="keyword">import</span> cytnx</div><div class="line">A =  <a class="code" href="classcytnx_1_1Tensor.html">cytnx.Tensor</a>([3,4,5],dtype=cytnx.Type.Double,device=cytnx.Device.cpu)</div></div><!-- fragment --><h3>1. All the Storage and Tensor can now have mulitple type support.</h3>
<p>The avaliable types are :</p>
<table class="doxtable">
<tr>
<th>cytnx type </th><th>c++ type </th><th>Type object  </th></tr>
<tr>
<td>cytnx_double </td><td>double </td><td>Type.Double </td></tr>
<tr>
<td>cytnx_float </td><td>float </td><td>Type.Float </td></tr>
<tr>
<td>cytnx_uint64 </td><td>uint64_t </td><td>Type.Uint64 </td></tr>
<tr>
<td>cytnx_uint32 </td><td>uint32_t </td><td>Type.Uint32 </td></tr>
<tr>
<td>cytnx_uint16 </td><td>uint16_t </td><td>Type.Uint16 </td></tr>
<tr>
<td>cytnx_int64 </td><td>int64_t </td><td>Type.Int64 </td></tr>
<tr>
<td>cytnx_int32 </td><td>int32_t </td><td>Type.Int32 </td></tr>
<tr>
<td>cytnx_int16 </td><td>int16_t </td><td>Type.Int16 </td></tr>
<tr>
<td>cytnx_complex128 </td><td>std::complex&lt;double&gt; </td><td>Type.ComplexDouble </td></tr>
<tr>
<td>cytnx_complex64 </td><td>std::complex&lt;float&gt; </td><td>Type.ComplexFloat </td></tr>
<tr>
<td>cytnx_bool </td><td>bool </td><td>Type.Bool </td></tr>
</table>
<h3>2. Multiple devices support.</h3>
<ul>
<li>simple moving btwn CPU and GPU (see below)</li>
</ul>
<h2>Objects:</h2>
<ul>
<li><a class="el" href="classcytnx_1_1Storage.html">Storage </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Tensor.html">Tensor </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Bond.html">Bond </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Accessor.html">Accessor </a> [c++ only]</li>
<li><a class="el" href="classcytnx_1_1Symmetry.html">Symmetry </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1UniTensor.html">UniTensor </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Network.html">Network </a> [binded]</li>
</ul>
<h2>linear algebra functions:</h2>
<p>See <a class="el" href="namespacecytnx_1_1linalg.html">cytnx::linalg </a> for further details</p>
<table class="doxtable">
<tr>
<th>func </th><th>inplace </th><th>CPU </th><th>GPU </th><th>callby tn  </th></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#af36e9e20e4c7d74f2f6f838902482d98">Add</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a78477b25b3eed121847f1a13b878a925">Sub</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a2fc49876b7b53f6f6e97ce70f475f636">Mul</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc1940e0e7364299ea1481c81003ba13">Div</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aea125f2928934007725809426bb77e38">Cpr</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>+,+=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="">Tensor.Add_</a>) </td></tr>
<tr>
<td>-,-=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="">Tensor.Sub_</a>) </td></tr>
<tr>
<td>*,*=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="">Tensor.Mul_</a>) </td></tr>
<tr>
<td>/,/=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="">Tensor.Div_</a>) </td></tr>
<tr>
<td>== [tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="">Tensor.Cpr_</a>) </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#ac17f16959a4849ea91bce712d24d4e4e">Svd</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>*<a class="el" href="">Svd_truncate</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a351ad02f478ba8082ee79a37a2a8f108">Inv</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a26628db51e90867ddc050ab11a317a8d">Inv_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a470d0886432554a35ecaf961451c0806">Conj</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#adc3233bf8bc3eb6a435340f912412801">Conj_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aac38382cbc0e8202411c96a0ff636471">Exp</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#aaab08439dde94ee87939d07933ede6e3">Exp_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a5831918722e5d18f4eaf37834b8fbf77">Expf</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a5de1faf71c76cdc6b7fa5ba3a3e21bbb">Expf_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aa9f11ed24ef9684fb8d05c228e3852d6">Eigh</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a53feb441b4b1bd263714ed33e093728f">Matmul</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a5913f0bdd6cc130aeb927f42a874a149">Diag</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a460e44db6b3d5d2c30c2d2723ff8f788">Tensordot</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a3e8edc89fdabb9c0f9b342198a31798b">Outer</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#acf52cf0ddfbacfcbc9970a1e85560b66">Kron</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aa69a91a5651fce55380cf800c6030d73">Vectordot</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc68d62e804d6d3e86aeec00015b07cd">Tridiag</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
</table>
<p>*this is a high level linalg ^this is temporary disable</p>
<h2>Generators</h2>
<p>Tensor: <a class="el" href="namespacecytnx.html#ab8a79a03fb0465f3eb2641017f3f1755">zeros()</a>, <a class="el" href="namespacecytnx.html#a83fb7bbe73368751a0d0f535d4a10a33">ones()</a>, <a class="el" href="namespacecytnx.html#a733f9931141463bc8b7c61931ccf52c3">arange()</a></p>
<h2>Requirements</h2>
<ul>
<li>Boost v1.53+ [check_deleted, atomicadd, intrusive_ptr]</li>
<li>C++11</li>
<li>lapack</li>
<li>blas</li>
<li>gcc v4.8.5+ (recommand v6+) (required -std=c++11)</li>
</ul>
<p>[CUDA support]</p><ul>
<li>CUDA v10+</li>
<li>cuDNN</li>
</ul>
<p>[OpenMp support]</p><ul>
<li>openmp</li>
</ul>
<p>[Python]</p><ul>
<li>pybind11 2.2.4</li>
<li>numpy &gt;= 1.15</li>
</ul>
<h2>docker image with MKL</h2>
<p><a href="https://hub.docker.com/r/kaihsinwu/cytnx_mkl">https://hub.docker.com/r/kaihsinwu/cytnx_mkl</a></p>
<h3>To run:</h3>
<div class="fragment"><div class="line">$docker pull kaihsinwu/cytnx_mkl</div><div class="line">$docker run -ti kaihsinwu/cytnx_mkl</div></div><!-- fragment --><h3>Note:</h3>
<p>Once docker image is run, the user code can be compile (for example) with:</p>
<div class="fragment"><div class="line">$g++-6 -std=c++11 -O3 &lt;your.code.cpp&gt; /opt/cytnx/libcytnx.so</div></div><!-- fragment --><h2>compile</h2>
<p>1.) Set the flags to config the install inside make.inc There are 4 important flags: ICPC_Enable, OMP_Enable, GPU_Enable, MKL_Enable</p><ul>
<li><p class="startli">a. The default compiler is g++-6. Change "GCC" for the compiler on your system.</p>
<p class="startli">To use intel icpc compiler instead of default compiler, set "ICPC_Enable"=1.</p>
<p class="startli">[Note] You can only choose either icpc or gcc.</p><ul>
<li>In the case where ICPC_Enable=1, GCC will be ignore.</li>
<li>In the case where ICPC_Enable=1, set "ICPC" to the path of your icpc binary.</li>
<li>In the case where ICPC_Enable=0, "ICPC" will be ignored.</li>
</ul>
</li>
<li><p class="startli">b. To enable the GPU support, set "GPU_Enable" =1, otherwise =0.</p>
<p class="startli">[Note]</p><ul>
<li>if GPU_Enable=1, the "CUDA_PATH" should be set to the cuda directory on your system.</li>
<li>if GPU_Enable=0, the "CUDA_PATH" will be ignored.</li>
</ul>
</li>
<li>c. To enable the acceleration using OpenMP, set "OMP_Enable" =1, otherwise =0.</li>
<li>d. The default linalg library are using LAPACK and BLAS. To use intel MKL instead, set "MKL_enable" =1.</li>
</ul>
<p>2.) compile by running: </p><pre class="fragment">$make -Bj
</pre><p>3.) [Option] compile the python API </p><pre class="fragment">$make pyobj
</pre><h2>Some snippets:</h2>
<h3>Storage</h3>
<ul>
<li>Memory container with GPU/CPU support. maintain type conversions (type casting btwn Storages) and moving btwn devices.</li>
<li>Generic type object, the behavior is very similar to python. <div class="fragment"><div class="line">Storage A(400,Type.Double);</div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i=0;i&lt;400;i++)</div><div class="line">    A.<a class="code" href="classcytnx_1_1Tensor.html#a697b114d1df390ca558ea9211c2d683b">at</a>&lt;<span class="keywordtype">double</span>&gt;(i) = i;</div><div class="line"></div><div class="line">Storage B = A; <span class="comment">// A and B share same memory, this is similar as python </span></div><div class="line"></div><div class="line">Storage C = A.<a class="code" href="classcytnx_1_1Tensor.html#acf7f697a9434f9bc98a7d00a555ee982">to</a>(Device.cuda+0); </div></div><!-- fragment --></li>
</ul>
<h3>Tensor</h3>
<ul>
<li>A tensor, API very similar to numpy and pytorch.</li>
<li>simple moving btwn CPU and GPU: <div class="fragment"><div class="line">Tensor A({3,4},Type.Double,Device.cpu); <span class="comment">// create tensor on CPU (default)</span></div><div class="line">Tensor B({3,4},Type.Double,Device.cuda+0); <span class="comment">// create tensor on GPU with gpu-id=0</span></div><div class="line"></div><div class="line"></div><div class="line">Tensor C = B; <span class="comment">// C and B share same memory.</span></div><div class="line"></div><div class="line"><span class="comment">// move A to gpu</span></div><div class="line">Tensor D = A.<a class="code" href="classcytnx_1_1Tensor.html#acf7f697a9434f9bc98a7d00a555ee982">to</a>(Device.cuda+0);</div><div class="line"></div><div class="line"><span class="comment">// inplace move A to gpu</span></div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a114a31fbb8bf4a90f150b6a67e42183a">to_</a>(Device.cuda+0);</div></div><!-- fragment --></li>
<li>Type conversion in between avaliable: <div class="fragment"><div class="line">Tensor A({3,4},Type.Double);</div><div class="line">Tensor B = A.<a class="code" href="classcytnx_1_1Tensor.html#a7b996d3281e7375b29a7cfe4273b299f">astype</a>(Type.Uint64); <span class="comment">// cast double to uint64_t</span></div></div><!-- fragment --></li>
<li>vitual swap and permute. All the permute and swap will not change the underlying memory</li>
<li>Use Contiguous() when needed to actual moving the memory layout. <div class="fragment"><div class="line">Tensor A({3,4,5,2},Type.Double);</div><div class="line">A.permute_({0,3,1,2}); <span class="comment">// this will not change the memory, only the shape info is changed.</span></div><div class="line">cout &lt;&lt; A.is_contiguous() &lt;&lt; endl; <span class="comment">// this will be false!</span></div><div class="line"></div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a405470654ef4ef5fc0b1d24754a3daf9">contiguous_</a>(); <span class="comment">// call Configuous() to actually move the memory.</span></div><div class="line">cout &lt;&lt; A.is_contiguous() &lt;&lt; endl; <span class="comment">// this will be true!</span></div></div><!-- fragment --></li>
<li>access single element using .at <div class="fragment"><div class="line">Tensor A({3,4,5},Type.Double);</div><div class="line"><span class="keywordtype">double</span> val = A.<a class="code" href="classcytnx_1_1Tensor.html#a697b114d1df390ca558ea9211c2d683b">at</a>&lt;<span class="keywordtype">double</span>&gt;({0,2,2});</div></div><!-- fragment --></li>
<li>access elements with python slices similarity: <div class="fragment"><div class="line"><span class="keyword">typedef</span> Accessor ac;</div><div class="line">Tensor A({3,4,5},Type.Double);</div><div class="line">Tensor out = A.<a class="code" href="classcytnx_1_1Tensor.html#ad7b928e4cb89d40cbd99aefab9aa0075">get</a>({ac(0),ac::all(),ac::range(1,4)}); </div><div class="line"><span class="comment">// equivalent to python: out = A[0,:,1:4]    </span></div></div><!-- fragment --></li>
</ul>
<h2>Fast Examples</h2>
<pre class="fragment">See test.cpp for using C++ .
See test.py for using python  
</pre><h2>Developer</h2>
<pre class="fragment">Kai-Hsin Wu kaihsinwu@gmail.com </pre> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
