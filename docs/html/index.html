<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Cytnx: Cytnx</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="Icon_small.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Cytnx
   &#160;<span id="projectnumber">v0.5</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Cytnx </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Stable Version:</h2>
<p><a href="https://github.com/kaihsin/Cytnx/tree/v0.5.1a">v0.5.1 pre-release</a></p>
<h2>Current dev Version:</h2>
<p>v0.5.2</p>
<h2>What's new:</h2>
<p>v0.5.2</p><ol type="1">
<li>add Trace and Trace_ for CyTensor.</li>
<li>fix bug in Network.Launch does not return the output CyTensor</li>
<li>Add Network.PrintNet, and ostream support.</li>
</ol>
<p>v0.5.1a</p><ol type="1">
<li>add <a class="el" href="namespacecytnx_1_1linalg.html#a9cd2be179860bb4742ebe320fa063680" title="calculate the norm of a vector (or matrix in the future) ">Norm()</a> for CPU and GPU, add to call by Tn</li>
<li>add <a class="el" href="namespacecytnx_1_1linalg.html#a215dbfd2aa7ef898450de7afff726bca" title="dot product of two arrays. ">Dot()</a> for CPU and GPU, with unify API for Vec-Vec/Mat-Vec/Mat-Mat/Ten-Vec product.</li>
<li>add Tensor.rank()</li>
<li>[Feature] support Tensor &lt;-&gt; numpy.ndarray</li>
<li>add <a class="el" href="namespacecytnx_1_1random.html#a3e0802a4f83adb8b3c8837b53b9f5d94" title="Randomize the memory of a Storage with uniform distributon. ">random::Make_uniform()</a></li>
<li>Fix bug in Svd_truncate that will change the underlying block for contiguous CyTensor.</li>
<li>Fix bug in Tensor-&gt;numpy if the underlying Tensor is non-contiguous.</li>
<li>Add Eig.</li>
<li>Add real() imag() for Tensor.</li>
<li>Enhance python API, Storage &amp; Tensor are now iterable.</li>
<li>Fix buf in Conj and Conj_, for both C++ and python</li>
<li>Fix bug python inplace call return ID Conj_, Inv_, Exp_</li>
<li>Add Conj, Conj_ for CyTensor</li>
<li>Fix non-inplace Arithmetic for non-contiguous tensor.</li>
<li>Add [trial version] Trace.</li>
<li>Add Pow, Pow_.</li>
<li>Add Abs, Abs_ for cpu.</li>
</ol>
<p>v0.5.0a</p><ol type="1">
<li>Add .imag() .real() for Storage.</li>
<li>Add xlinalg under <a class="el" href="namespacecytnx__extension.html">cytnx_extension</a> for algebra on CyTensor</li>
<li>Add <a class="el" href="namespacecytnx__extension_1_1xlinalg.html#af9dfa33e6c7b8e48c4a4fe3bd62a940a">xlinalg::Svd()</a></li>
<li>Change <a class="el" href="namespacecytnx_1_1linalg.html#aeb91cc90ebd85069ea9e7808e64068a2" title="eigen-value decomposition for Hermitian matrix ">linalg::Eigh()</a> to match numpy</li>
<li>fix Diag uninitialize elemets bug</li>
<li>add <a class="el" href="namespacecytnx_1_1linalg.html#a003ad2d2784202e936ce4a16e850e17e" title="perform matrix exponential ">linalg::ExpH()</a></li>
<li>add <a class="el" href="namespacecytnx_1_1random.html#ac456ace342f35864d3a927b3ce9fccbd" title="Randomize the memory of a Storage with normal distributon. ">random::Make_normal()</a></li>
<li>add iTEBD example for both C++ and python @ example/iTEBD</li>
</ol>
<h2>Version log</h2>
<p>v0.4</p><ol type="1">
<li>remove Otimes, add Kron and Outer</li>
<li>Add Storage append, capacity, pre-alloc 32x address</li>
<li>Tensor can now allow redundant dimension (e.g. shape = (1,1,1,1,1...)</li>
<li>Add Storage.from_vector, directly convert the C++ vector to Storage</li>
<li>Add more intruisive way to get slices for Tensor in C++, using operator[]</li>
<li>Add Tensor.append for rank-1 Tensor</li>
<li>Add <a class="el" href="namespacecytnx_1_1linalg.html#aac38382cbc0e8202411c96a0ff636471" title="Exponential all the element in Tensor. ">Exp()</a> <a class="el" href="namespacecytnx_1_1linalg.html#a5831918722e5d18f4eaf37834b8fbf77" title="Exponential all the element in Tensor. ">Expf()</a> <a class="el" href="namespacecytnx_1_1linalg.html#aaab08439dde94ee87939d07933ede6e3" title="inplace perform Exponential on all the element in Tensor. ">Exp_()</a> <a class="el" href="namespacecytnx_1_1linalg.html#a5de1faf71c76cdc6b7fa5ba3a3e21bbb" title="inplace perform Exponential on all the element in Tensor. ">Expf_()</a></li>
<li>Change UniTensor to CyTensor</li>
<li>Guarded CyTensor, Bond, Symmetry and Network class with <a class="el" href="namespacecytnx__extension.html">cytnx_extension</a> namespace (<a class="el" href="namespacecytnx__extension.html">cytnx_extension</a> submodule in python).</li>
</ol>
<h2>Feature:</h2>
<h3>Python x C++</h3>
<p>Benefit from both side. One can do simple prototype on python side and easy transfer to C++ with small effort!</p>
<div class="fragment"><div class="line"><span class="comment">// c++ version:</span></div><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="cytnx_8hpp.html">cytnx.hpp</a>&quot;</span></div><div class="line"><a class="code" href="classcytnx_1_1Tensor.html">cytnx::Tensor</a> A({3,4,5},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">cytnx::Type</a>.Double,<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">cytnx::Device</a>.cpu)</div></div><!-- fragment --><div class="fragment"><div class="line"><span class="comment"># python version:</span></div><div class="line"><span class="keyword">import</span> cytnx</div><div class="line">A =  <a class="code" href="classcytnx_1_1Tensor.html">cytnx.Tensor</a>([3,4,5],dtype=cytnx.Type.Double,device=cytnx.Device.cpu)</div></div><!-- fragment --><h3>1. All the Storage and Tensor can now have mulitple type support.</h3>
<p>The avaliable types are :</p>
<table class="doxtable">
<tr>
<th>cytnx type </th><th>c++ type </th><th>Type object  </th></tr>
<tr>
<td>cytnx_double </td><td>double </td><td>Type.Double </td></tr>
<tr>
<td>cytnx_float </td><td>float </td><td>Type.Float </td></tr>
<tr>
<td>cytnx_uint64 </td><td>uint64_t </td><td>Type.Uint64 </td></tr>
<tr>
<td>cytnx_uint32 </td><td>uint32_t </td><td>Type.Uint32 </td></tr>
<tr>
<td>cytnx_uint16 </td><td>uint16_t </td><td>Type.Uint16 </td></tr>
<tr>
<td>cytnx_int64 </td><td>int64_t </td><td>Type.Int64 </td></tr>
<tr>
<td>cytnx_int32 </td><td>int32_t </td><td>Type.Int32 </td></tr>
<tr>
<td>cytnx_int16 </td><td>int16_t </td><td>Type.Int16 </td></tr>
<tr>
<td>cytnx_complex128 </td><td>std::complex&lt;double&gt; </td><td>Type.ComplexDouble </td></tr>
<tr>
<td>cytnx_complex64 </td><td>std::complex&lt;float&gt; </td><td>Type.ComplexFloat </td></tr>
<tr>
<td>cytnx_bool </td><td>bool </td><td>Type.Bool </td></tr>
</table>
<h3>2. Multiple devices support.</h3>
<ul>
<li>simple moving btwn CPU and GPU (see below)</li>
</ul>
<h2>Objects:</h2>
<ul>
<li><a class="el" href="classcytnx_1_1Storage.html">Storage </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Tensor.html">Tensor </a> [binded]</li>
<li><a class="el" href="">Bond </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Accessor.html">Accessor </a> [c++ only]</li>
<li><a class="el" href="">Symmetry </a> [binded]</li>
<li><a class="el" href="">CyTensor </a> [binded]</li>
<li><a class="el" href="">Network </a> [binded]</li>
</ul>
<h2>linear algebra functions:</h2>
<p>See <a class="el" href="namespacecytnx_1_1linalg.html">cytnx::linalg </a> for further details</p>
<table class="doxtable">
<tr>
<th>func </th><th>inplace </th><th>CPU </th><th>GPU </th><th>callby tn  </th></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#af36e9e20e4c7d74f2f6f838902482d98">Add</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a78477b25b3eed121847f1a13b878a925">Sub</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a2fc49876b7b53f6f6e97ce70f475f636">Mul</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc1940e0e7364299ea1481c81003ba13">Div</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aea125f2928934007725809426bb77e38">Cpr</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>+,+=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#ac7ab4b9ee38f619a60c26f203539db65">Tensor.Add_</a>) </td></tr>
<tr>
<td>-,-=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#a3482ddb0ebd4ae6585dcea7c96702e0c">Tensor.Sub_</a>) </td></tr>
<tr>
<td>*,*=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#a92461c94266f7185da6f8fd35d1ffee4">Tensor.Mul_</a>) </td></tr>
<tr>
<td>/,/=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#a030748d3e2003f655b1cf73fe0d55a63">Tensor.Div_</a>) </td></tr>
<tr>
<td>== [tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#ae9e9ea98664899db1ce86b88e136595e">Tensor.Cpr_</a>) </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#ac17f16959a4849ea91bce712d24d4e4e">Svd</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a37887f24c0050785a9675ecbb7601952">Svd_truncate</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a351ad02f478ba8082ee79a37a2a8f108">Inv</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a26628db51e90867ddc050ab11a317a8d">Inv_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a470d0886432554a35ecaf961451c0806">Conj</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#adc3233bf8bc3eb6a435340f912412801">Conj_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aac38382cbc0e8202411c96a0ff636471">Exp</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#aaab08439dde94ee87939d07933ede6e3">Exp_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a5831918722e5d18f4eaf37834b8fbf77">Expf</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a5de1faf71c76cdc6b7fa5ba3a3e21bbb">Expf_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a003ad2d2784202e936ce4a16e850e17e">ExpH</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aeb91cc90ebd85069ea9e7808e64068a2">Eigh</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a53feb441b4b1bd263714ed33e093728f">Matmul</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a5913f0bdd6cc130aeb927f42a874a149">Diag</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a460e44db6b3d5d2c30c2d2723ff8f788">Tensordot</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a3e8edc89fdabb9c0f9b342198a31798b">Outer</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#acf52cf0ddfbacfcbc9970a1e85560b66">Kron</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a9cd2be179860bb4742ebe320fa063680">Norm</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aa69a91a5651fce55380cf800c6030d73">Vectordot</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc68d62e804d6d3e86aeec00015b07cd">Tridiag</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a215dbfd2aa7ef898450de7afff726bca">Dot</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a1137ae19828f715399d90634e4a71ba3">Eig</a> </td><td>x </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a2919ef6c163a54360071c286df3fb92e">Pow</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a269ef291355dfee4421fa55ad0247aad">Pow_</a> </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td><a class="el" href="">abs</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a30687e6240a889c04637b33ef90c2525">Abs_</a> </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a02d7105bf16d23128687ba92fb231b54">Trace</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
</table>
<p>* this is a high level linalg</p>
<p>^ this is temporary disable</p>
<h2>Container Generators</h2>
<p>Tensor: <a class="el" href="namespacecytnx.html#ab8a79a03fb0465f3eb2641017f3f1755">zeros()</a>, <a class="el" href="namespacecytnx.html#a83fb7bbe73368751a0d0f535d4a10a33">ones()</a>, <a class="el" href="namespacecytnx.html#a733f9931141463bc8b7c61931ccf52c3">arange()</a></p>
<h2>Random</h2>
<p>See <a class="el" href="namespacecytnx_1_1random.html">cytnx::random </a> for further details</p>
<table class="doxtable">
<tr>
<th>func </th><th>Tn </th><th>Stor </th><th>CPU </th><th>GPU  </th></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1random.html#ac456ace342f35864d3a927b3ce9fccbd">Make_normal</a> </td><td>Y </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>^<a class="el" href="namespacecytnx_1_1random.html#a11ab0d554038b6339bf33a67c4977b70">normal</a> </td><td>Y </td><td>x </td><td>Y </td><td>Y </td></tr>
</table>
<ul>
<li>this is initializer</li>
</ul>
<p>^ this is generator</p>
<p>[Note] The difference of initializer and generator is that initializer is used to initialize the Tensor, and generator generates a new Tensor.</p>
<h2>Requirements</h2>
<ul>
<li>Boost v1.53+ [check_deleted, atomicadd, intrusive_ptr]</li>
<li>C++11</li>
<li>lapack</li>
<li>blas</li>
<li>gcc v4.8.5+ (recommand v6+) (required -std=c++11)</li>
</ul>
<p>[CUDA support]</p><ul>
<li>CUDA v10+</li>
<li>cuDNN</li>
</ul>
<p>[OpenMp support]</p><ul>
<li>openmp</li>
</ul>
<p>[Python]</p><ul>
<li>pybind11 2.2.4</li>
<li>numpy &gt;= 1.15</li>
</ul>
<h2>conda install</h2>
<p>[Currently Linux only]</p>
<p>without CUDA</p><ul>
<li>python 3.6: conda install -c kaihsinwu cytnx_36</li>
<li>python 3.7: conda install -c kaihsinwu cytnx_37</li>
</ul>
<p>with CUDA</p><ul>
<li>python 3.6: conda install -c kaihsinwu cytnx_cuda_36</li>
<li>python 3.7: conda install -c kaihsinwu cytnx_cuda_37</li>
</ul>
<h2>docker image with MKL</h2>
<p><a href="https://hub.docker.com/r/kaihsinwu/cytnx_mkl">https://hub.docker.com/r/kaihsinwu/cytnx_mkl</a></p>
<h3>To run:</h3>
<div class="fragment"><div class="line">$docker pull kaihsinwu/cytnx_mkl</div><div class="line">$docker run -ti kaihsinwu/cytnx_mkl</div></div><!-- fragment --><h3>Note:</h3>
<p>Once docker image is run, the user code can be compile (for example) with:</p>
<div class="fragment"><div class="line">$g++-6 -std=c++11 -O3 &lt;your.code.cpp&gt; /opt/cytnx/libcytnx.so</div></div><!-- fragment --><h2>compile</h2>
<p>1.) create a build folder, and cd to the folder $mkdir build</p>
<p>$cd build</p>
<p>2.) resolving the dependency </p><pre class="fragment">$cmake [flags (optional)] &lt;Cytnx repo folder&gt;

[Note] there are several customize flags format as (-D&lt;flag name&gt;).

* USE_ICPC (default = off)

    The default compiler is g++-6. 

* USE_CUDA (default = off)

    If USE_CUDA=1, the code will compile with GPU support.

* USE_MKL (default = off)

    If USE_MKL=off, the code will compile with system LAPACK/BLAS library. 

* CMAKE_INSTALL_PREFIX (default is /usr/local)

    Set the install target path.
</pre><p>3.) compile by running: </p><pre class="fragment">$make -Bj4
</pre><p>4.) install to the target path. </p><pre class="fragment">$make install
</pre><h2>Some snippets:</h2>
<h3>Storage</h3>
<ul>
<li>Memory container with GPU/CPU support. maintain type conversions (type casting btwn Storages) and moving btwn devices.</li>
<li>Generic type object, the behavior is very similar to python. <div class="fragment"><div class="line">Storage A(400,<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i=0;i&lt;400;i++)</div><div class="line">    A.<a class="code" href="classcytnx_1_1Tensor.html#a697b114d1df390ca558ea9211c2d683b">at</a>&lt;<span class="keywordtype">double</span>&gt;(i) = i;</div><div class="line"></div><div class="line">Storage B = A; <span class="comment">// A and B share same memory, this is similar as python </span></div><div class="line"></div><div class="line">Storage C = A.<a class="code" href="classcytnx_1_1Tensor.html#acf7f697a9434f9bc98a7d00a555ee982">to</a>(<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0); </div></div><!-- fragment --></li>
</ul>
<h3>Tensor</h3>
<ul>
<li>A tensor, API very similar to numpy and pytorch.</li>
<li>simple moving btwn CPU and GPU: <div class="fragment"><div class="line">Tensor A({3,4},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double,<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cpu); <span class="comment">// create tensor on CPU (default)</span></div><div class="line">Tensor B({3,4},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double,<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0); <span class="comment">// create tensor on GPU with gpu-id=0</span></div><div class="line"></div><div class="line"></div><div class="line">Tensor C = B; <span class="comment">// C and B share same memory.</span></div><div class="line"></div><div class="line"><span class="comment">// move A to gpu</span></div><div class="line">Tensor D = A.<a class="code" href="classcytnx_1_1Tensor.html#acf7f697a9434f9bc98a7d00a555ee982">to</a>(<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0);</div><div class="line"></div><div class="line"><span class="comment">// inplace move A to gpu</span></div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a114a31fbb8bf4a90f150b6a67e42183a">to_</a>(<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0);</div></div><!-- fragment --></li>
<li>Type conversion in between avaliable: <div class="fragment"><div class="line">Tensor A({3,4},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line">Tensor B = A.<a class="code" href="classcytnx_1_1Tensor.html#a7b996d3281e7375b29a7cfe4273b299f">astype</a>(<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Uint64); <span class="comment">// cast double to uint64_t</span></div></div><!-- fragment --></li>
<li>vitual swap and permute. All the permute and swap will not change the underlying memory</li>
<li>Use Contiguous() when needed to actual moving the memory layout. <div class="fragment"><div class="line">Tensor A({3,4,5,2},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a4239ec84bd34f634a822eeff416f4d63">permute_</a>({0,3,1,2}); <span class="comment">// this will not change the memory, only the shape info is changed.</span></div><div class="line">cout &lt;&lt; A.<a class="code" href="classcytnx_1_1Tensor.html#a6a1d9ed962b0e9a484e2bb0de15eb76d">is_contiguous</a>() &lt;&lt; endl; <span class="comment">// this will be false!</span></div><div class="line"></div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a3d4342299e6951dae13b6136b89f1d53">contiguous_</a>(); <span class="comment">// call Configuous() to actually move the memory.</span></div><div class="line">cout &lt;&lt; A.<a class="code" href="classcytnx_1_1Tensor.html#a6a1d9ed962b0e9a484e2bb0de15eb76d">is_contiguous</a>() &lt;&lt; endl; <span class="comment">// this will be true!</span></div></div><!-- fragment --></li>
<li>access single element using .at <div class="fragment"><div class="line">Tensor A({3,4,5},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line"><span class="keywordtype">double</span> val = A.<a class="code" href="classcytnx_1_1Tensor.html#a697b114d1df390ca558ea9211c2d683b">at</a>&lt;<span class="keywordtype">double</span>&gt;({0,2,2});</div></div><!-- fragment --></li>
<li>access elements with python slices similarity: <div class="fragment"><div class="line"><span class="keyword">typedef</span> Accessor <a class="code" href="namespacecytnx__extension.html#ae37352951e39124a1174b3f551f72897">ac</a>;</div><div class="line">Tensor A({3,4,5},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line">Tensor out = A.<a class="code" href="classcytnx_1_1Tensor.html#ad7b928e4cb89d40cbd99aefab9aa0075">get</a>({<a class="code" href="namespacecytnx__extension.html#ae37352951e39124a1174b3f551f72897">ac</a>(0),ac::all(),ac::range(1,4)}); </div><div class="line"><span class="comment">// equivalent to python: out = A[0,:,1:4]    </span></div></div><!-- fragment --></li>
</ul>
<h2>Fast Examples</h2>
<pre class="fragment">See test.cpp for using C++ .
See test.py for using python  
</pre><h2>Developer</h2>
<pre class="fragment">Kai-Hsin Wu kaihsinwu@gmail.com </pre> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
