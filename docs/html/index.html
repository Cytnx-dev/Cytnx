<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Cytnx: Cytnx</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="Icon_small.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Cytnx
   &#160;<span id="projectnumber">v0.5.2</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Cytnx </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Stable Version:</h2>
<p><a href="https://github.com/kaihsin/Cytnx/tree/v0.5.2a">v0.5.2 pre-release</a></p>
<h2>Current dev Version:</h2>
<p>v0.5.3</p>
<h2>What's new:</h2>
<p>v0.5.3</p><ol type="1">
<li>Add xlinalg.QR</li>
<li>enhance hosvd.</li>
<li>Fix bug in <a class="el" href="namespacecytnx_1_1linalg.html#a3ded6435c7b47eb348d03783bfe72611" title="Elementwise absolute value. ">cytnx.linalg.Abs</a> truncate the floating point part.</li>
<li>Add example for HOTRG</li>
<li>Add example for iDMRG</li>
<li>Add CyTensor.truncate/truncate.</li>
<li>Add linalg::Sum.</li>
<li>Complete set_elem for sparse CyTensor dispatch in binding.</li>
<li>[Important] Change Inv/Inv_ to InvM/InvM_ for matrix inverse.</li>
</ol>
<p>v0.5.2</p><ol type="1">
<li>add Trace and Trace_ for CyTensor.</li>
<li>fix bug in Network.Launch does not return the output CyTensor</li>
<li>Add Network.PrintNet, and ostream support.</li>
<li>Add Network.Diagram() for plot the tensor network diagram (python only)</li>
<li>Add support for floating type Vectordot on GPU.</li>
<li>Fix bug in to from Anytype to ComplexFloat.</li>
<li>Add QR for CPU.</li>
<li>Add <a class="el" href="namespacecytnx.html#af8aa7e619c030f80c54e4d26b576484b" title="create an square rank-2 Tensor with all diagonal to be one. ">identity()</a> and it's alias function <a class="el" href="namespacecytnx.html#affbd9073156ce86d5e5c03742603c631" title="create an square rank-2 Tensor with all diagonal to be one. ">eye()</a>.</li>
<li>Add physics namespace/submodule</li>
<li>Add <a class="el" href="namespacecytnx_1_1physics.html#a9f018f04ccd068e899a8fcfe2e9b82d4" title="create Spin-S representation matrix. ">physics::spin()</a> for generating Spin-S representation.</li>
<li>Add <a class="el" href="namespacecytnx_1_1physics.html#a1c6ebefa40751712dfb8976b015f48d6" title="create Pauli matrix. ">physics::pauli()</a> for pauli matrix.</li>
<li>Add <a class="el" href="namespacecytnx__extension_1_1xlinalg.html#aa6039e19e0dd0e7b0e73032050a8aec3">ExpM()</a> for generic matrix (CPU only)</li>
<li>Fix bug in python slice, and reverse range slice.</li>
<li>Enhance optional Kron padding scheme</li>
<li>Fix bug in CyTensor contract/Contract(A,B) for tensors with no common label</li>
<li>Enhance error message in Network</li>
<li>Add <a class="el" href="namespacecytnx_1_1linalg.html#a25b5f2fdfb550aabc9cc41daa963e1fb" title="get the minimum element. ">Min()</a>, <a class="el" href="namespacecytnx_1_1linalg.html#a93ef263824e2dcdab4832ea628959d19" title="get the maximum element. ">Max()</a> (CPU only)</li>
<li>Fix bug in Abs.</li>
<li>Fix bug in Div i32td.</li>
<li>[Feature] Add optimal contraction order calculation in Network</li>
<li>Fix SparseCyTensor contiguous address wrong calculation.</li>
<li>Support at() directly from SparseCyTensor.</li>
<li>Add Transpose, Dagger to CyTensor. For tagged CyTensor, Transpose/Dagger will reverse the direction of all bonds.</li>
<li>Add xlinalg.Svd, xlinalg.Svd_truncate support for tagged CyTensor.</li>
<li>Fix redundant print in optimal contraction order</li>
<li>Add CyTensor.tag() for DenseCyTensor (regular type) directly convert to CyTensor with direction (tagged type)</li>
<li>Add SparseCyTensor.at (currently only floating point type)</li>
<li>SparseCyTensor.ele_exists.</li>
<li>SparseCyTensor.Transpose, Conj.</li>
<li>Symmetry.reverse_rule, Bond.calc_reverse_qnums</li>
<li>Fix Tensor.numpy from GPU bug.</li>
<li>Fix Tensor.setitem/getitem pybind bug.</li>
<li>SparseCyTensor.get_elem/set_elem (currently floating type only (complex))</li>
<li>Add xlinalg::ExpH, xlinalg::ExpM, xlinalg::Trace (ovld of CyTensor.Trace)</li>
<li>support Mul/Div operation on SparseCyTensor</li>
<li>Add Tensor.flatten()</li>
<li>Add Network.Savefile. Network.PutCyTensors</li>
<li>[Feature] Tensor can now use unify operator[] to get and set elements as python API</li>
<li>fix ambiguous error message in Tensor arithmetic.</li>
<li>fix bug in xlinalg::Svd</li>
<li>fix bug in physics::pauli</li>
<li>fix bug in CyTensor.set_label checking element.</li>
<li>Add xlinalg::Hosvd (currently CyTensor only)</li>
<li>change argument of init CyTensor rowrank-&gt;Rowrank</li>
<li>Add PESS example</li>
<li>Add support for Norm to generic rank-N Tensor</li>
<li>Add @ operator in python API for shorthand of linalg::Dot</li>
<li>Add DMRG example</li>
<li>C++ API can now have accessor.size() &lt; rank()</li>
<li>Remove redundant output of Inv.</li>
<li>Add Pow, Pow_ for CyTensor.</li>
<li>Add Symmetry.Save/Load</li>
<li>Symmetry/Tensor/Storage/Bond/CyTensor Save/Load re-invented for more simple usage</li>
</ol>
<h2>Version log</h2>
<p>v0.5.1a</p><ol type="1">
<li>add <a class="el" href="namespacecytnx_1_1linalg.html#a9cd2be179860bb4742ebe320fa063680" title="calculate the norm of a tensor. ">Norm()</a> for CPU and GPU, add to call by Tn</li>
<li>add <a class="el" href="namespacecytnx_1_1linalg.html#a215dbfd2aa7ef898450de7afff726bca" title="dot product of two arrays. ">Dot()</a> for CPU and GPU, with unify API for Vec-Vec/Mat-Vec/Mat-Mat/Ten-Vec product.</li>
<li>add Tensor.rank()</li>
<li>[Feature] support Tensor &lt;-&gt; numpy.ndarray</li>
<li>add <a class="el" href="namespacecytnx_1_1random.html#a3e0802a4f83adb8b3c8837b53b9f5d94" title="Randomize the memory of a Storage with uniform distributon. ">random::Make_uniform()</a></li>
<li>Fix bug in Svd_truncate that will change the underlying block for contiguous CyTensor.</li>
<li>Fix bug in Tensor-&gt;numpy if the underlying Tensor is non-contiguous.</li>
<li>Add Eig.</li>
<li>Add real() imag() for Tensor.</li>
<li>Enhance python API, Storage &amp; Tensor are now iterable.</li>
<li>Fix buf in Conj and Conj_, for both C++ and python</li>
<li>Fix bug python inplace call return ID Conj_, Inv_, Exp_</li>
<li>Add Conj, Conj_ for CyTensor</li>
<li>Fix non-inplace Arithmetic for non-contiguous tensor.</li>
<li>Add [trial version] Trace.</li>
<li>Add Pow, Pow_.</li>
<li>Add Abs, Abs_ for cpu.</li>
</ol>
<p>v0.5.0a</p><ol type="1">
<li>Add .imag() .real() for Storage.</li>
<li>Add xlinalg under <a class="el" href="namespacecytnx__extension.html">cytnx_extension</a> for algebra on CyTensor</li>
<li>Add <a class="el" href="namespacecytnx__extension_1_1xlinalg.html#af9dfa33e6c7b8e48c4a4fe3bd62a940a">xlinalg::Svd()</a></li>
<li>Change <a class="el" href="namespacecytnx_1_1linalg.html#aeb91cc90ebd85069ea9e7808e64068a2" title="eigen-value decomposition for Hermitian matrix ">linalg::Eigh()</a> to match numpy</li>
<li>fix Diag uninitialize elemets bug</li>
<li>add <a class="el" href="namespacecytnx__extension_1_1xlinalg.html#a836b450886c671a08c877451c54cee34">linalg::ExpH()</a></li>
<li>add <a class="el" href="namespacecytnx_1_1random.html#ac456ace342f35864d3a927b3ce9fccbd" title="Randomize the memory of a Storage with normal distributon. ">random::Make_normal()</a></li>
<li>add iTEBD example for both C++ and python @ example/iTEBD</li>
</ol>
<pre class="fragment">v0.4
1. remove Otimes, add Kron and Outer 
2. Add Storage append, capacity, pre-alloc 32x address
3. Tensor can now allow redundant dimension (e.g. shape = (1,1,1,1,1...) 
4. Add Storage.from_vector, directly convert the C++ vector to Storage
5. Add more intruisive way to get slices for Tensor in C++, using operator[]
6. Add Tensor.append for rank-1 Tensor    
7. Add Exp() Expf() Exp\_() Expf\_()
8. Change UniTensor to CyTensor 
9. Guarded CyTensor, Bond, Symmetry and Network class with cytnx_extension namespace (cytnx_extension submodule in python).  
</pre><h2>Feature:</h2>
<h3>Python x C++</h3>
<p>Benefit from both side. One can do simple prototype on python side and easy transfer to C++ with small effort!</p>
<div class="fragment"><div class="line"><span class="comment">// c++ version:</span></div><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="cytnx_8hpp.html">cytnx.hpp</a>&quot;</span></div><div class="line"><a class="code" href="classcytnx_1_1Tensor.html">cytnx::Tensor</a> A({3,4,5},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">cytnx::Type</a>.Double,<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">cytnx::Device</a>.cpu)</div></div><!-- fragment --><div class="fragment"><div class="line"><span class="comment"># python version:</span></div><div class="line"><span class="keyword">import</span> cytnx</div><div class="line">A =  <a class="code" href="classcytnx_1_1Tensor.html">cytnx.Tensor</a>([3,4,5],dtype=cytnx.Type.Double,device=cytnx.Device.cpu)</div></div><!-- fragment --><h3>1. All the Storage and Tensor can now have mulitple type support.</h3>
<p>The avaliable types are :</p>
<table class="doxtable">
<tr>
<th>cytnx type </th><th>c++ type </th><th>Type object  </th></tr>
<tr>
<td>cytnx_double </td><td>double </td><td>Type.Double </td></tr>
<tr>
<td>cytnx_float </td><td>float </td><td>Type.Float </td></tr>
<tr>
<td>cytnx_uint64 </td><td>uint64_t </td><td>Type.Uint64 </td></tr>
<tr>
<td>cytnx_uint32 </td><td>uint32_t </td><td>Type.Uint32 </td></tr>
<tr>
<td>cytnx_uint16 </td><td>uint16_t </td><td>Type.Uint16 </td></tr>
<tr>
<td>cytnx_int64 </td><td>int64_t </td><td>Type.Int64 </td></tr>
<tr>
<td>cytnx_int32 </td><td>int32_t </td><td>Type.Int32 </td></tr>
<tr>
<td>cytnx_int16 </td><td>int16_t </td><td>Type.Int16 </td></tr>
<tr>
<td>cytnx_complex128 </td><td>std::complex&lt;double&gt; </td><td>Type.ComplexDouble </td></tr>
<tr>
<td>cytnx_complex64 </td><td>std::complex&lt;float&gt; </td><td>Type.ComplexFloat </td></tr>
<tr>
<td>cytnx_bool </td><td>bool </td><td>Type.Bool </td></tr>
</table>
<h3>2. Multiple devices support.</h3>
<ul>
<li>simple moving btwn CPU and GPU (see below)</li>
</ul>
<h2>Objects:</h2>
<ul>
<li><a class="el" href="classcytnx_1_1Storage.html">Storage </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Tensor.html">Tensor </a> [binded]</li>
<li><a class="el" href="classcytnx__extension_1_1Bond.html">Bond </a> [binded]</li>
<li><a class="el" href="classcytnx_1_1Accessor.html">Accessor </a> [c++ only]</li>
<li><a class="el" href="classcytnx__extension_1_1Symmetry.html">Symmetry </a> [binded]</li>
<li><a class="el" href="classcytnx__extension_1_1CyTensor.html">CyTensor </a> [binded]</li>
<li><a class="el" href="classcytnx__extension_1_1Network.html">Network </a> [binded]</li>
</ul>
<h2>linear algebra functions:</h2>
<p>See <a class="el" href="namespacecytnx_1_1linalg.html">cytnx::linalg </a> for further details</p>
<table class="doxtable">
<tr>
<th>func </th><th>inplace </th><th>CPU </th><th>GPU </th><th>callby tn  </th></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#af36e9e20e4c7d74f2f6f838902482d98">Add</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a78477b25b3eed121847f1a13b878a925">Sub</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a2fc49876b7b53f6f6e97ce70f475f636">Mul</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc1940e0e7364299ea1481c81003ba13">Div</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aea125f2928934007725809426bb77e38">Cpr</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>+,+=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#ac7ab4b9ee38f619a60c26f203539db65">Tensor.Add_</a>) </td></tr>
<tr>
<td>-,-=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#a3482ddb0ebd4ae6585dcea7c96702e0c">Tensor.Sub_</a>) </td></tr>
<tr>
<td>*,*=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#a92461c94266f7185da6f8fd35d1ffee4">Tensor.Mul_</a>) </td></tr>
<tr>
<td>/,/=[tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#a030748d3e2003f655b1cf73fe0d55a63">Tensor.Div_</a>) </td></tr>
<tr>
<td>== [tn]</td><td>x </td><td>Y </td><td>Y </td><td>Y (<a class="el" href="classcytnx_1_1Tensor.html#ae9e9ea98664899db1ce86b88e136595e">Tensor.Cpr_</a>) </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#ac17f16959a4849ea91bce712d24d4e4e">Svd</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a37887f24c0050785a9675ecbb7601952">Svd_truncate</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#af819d2fc4522d2a6287aa16dfbe3f787">InvM</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#ac414e561888051c1fbacde18c9039fbf">InvM_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a470d0886432554a35ecaf961451c0806">Conj</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#adc3233bf8bc3eb6a435340f912412801">Conj_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aac38382cbc0e8202411c96a0ff636471">Exp</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#aaab08439dde94ee87939d07933ede6e3">Exp_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a5831918722e5d18f4eaf37834b8fbf77">Expf</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a5de1faf71c76cdc6b7fa5ba3a3e21bbb">Expf_</a> </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a003ad2d2784202e936ce4a16e850e17e">ExpH</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a671e57fbd0efcae9e438130ace6bf1cd">ExpM</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aeb91cc90ebd85069ea9e7808e64068a2">Eigh</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a53feb441b4b1bd263714ed33e093728f">Matmul</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a5913f0bdd6cc130aeb927f42a874a149">Diag</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a460e44db6b3d5d2c30c2d2723ff8f788">Tensordot</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a3e8edc89fdabb9c0f9b342198a31798b">Outer</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc22428de7f69d2afc9a27fca76bfe15">Kron</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a9cd2be179860bb4742ebe320fa063680">Norm</a> </td><td>x </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#aa69a91a5651fce55380cf800c6030d73">Vectordot</a> </td><td>x </td><td>Y </td><td>.Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#abc68d62e804d6d3e86aeec00015b07cd">Tridiag</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a215dbfd2aa7ef898450de7afff726bca">Dot</a> </td><td>x </td><td>Y </td><td>Y </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a1137ae19828f715399d90634e4a71ba3">Eig</a> </td><td>x </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a2919ef6c163a54360071c286df3fb92e">Pow</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a269ef291355dfee4421fa55ad0247aad">Pow_</a> </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a3ded6435c7b47eb348d03783bfe72611">Abs</a> </td><td><a class="el" href="namespacecytnx_1_1linalg.html#a30687e6240a889c04637b33ef90c2525">Abs_</a> </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a44a1aaacc828eb5202eb912f787626b8">QR</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a25b5f2fdfb550aabc9cc41daa963e1fb">Min</a> </td><td>x </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td><a class="el" href="namespacecytnx_1_1linalg.html#a93ef263824e2dcdab4832ea628959d19">Max</a> </td><td>x </td><td>Y </td><td>N </td><td>Y </td></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1linalg.html#a02d7105bf16d23128687ba92fb231b54">Trace</a> </td><td>x </td><td>Y </td><td>N </td><td>N </td></tr>
</table>
<p>* this is a high level linalg</p>
<p>^ this is temporary disable</p>
<p>. this is floating point type only</p>
<h2>Container Generators</h2>
<p>Tensor: <a class="el" href="namespacecytnx.html#ab8a79a03fb0465f3eb2641017f3f1755">zeros()</a>, <a class="el" href="namespacecytnx.html#a83fb7bbe73368751a0d0f535d4a10a33">ones()</a>, <a class="el" href="namespacecytnx.html#a733f9931141463bc8b7c61931ccf52c3">arange()</a>, <a class="el" href="namespacecytnx.html#af8aa7e619c030f80c54e4d26b576484b">identity()</a>, <a class="el" href="namespacecytnx.html#affbd9073156ce86d5e5c03742603c631">eye()</a>,</p>
<h2>Physics Category</h2>
<p>Tensor: <a class="el" href="namespacecytnx_1_1physics.html#a9f018f04ccd068e899a8fcfe2e9b82d4">spin()</a> <a class="el" href="namespacecytnx_1_1physics.html#a1c6ebefa40751712dfb8976b015f48d6">pauli()</a></p>
<h2>Random</h2>
<p>See <a class="el" href="namespacecytnx_1_1random.html">cytnx::random </a> for further details</p>
<table class="doxtable">
<tr>
<th>func </th><th>Tn </th><th>Stor </th><th>CPU </th><th>GPU  </th></tr>
<tr>
<td>*<a class="el" href="namespacecytnx_1_1random.html#ac456ace342f35864d3a927b3ce9fccbd">Make_normal</a> </td><td>Y </td><td>Y </td><td>Y </td><td>Y </td></tr>
<tr>
<td>^<a class="el" href="namespacecytnx_1_1random.html#a11ab0d554038b6339bf33a67c4977b70">normal</a> </td><td>Y </td><td>x </td><td>Y </td><td>Y </td></tr>
</table>
<ul>
<li>this is initializer</li>
</ul>
<p>^ this is generator</p>
<p>[Note] The difference of initializer and generator is that initializer is used to initialize the Tensor, and generator generates a new Tensor.</p>
<h2>Requirements</h2>
<ul>
<li>Boost v1.53+ [check_deleted, atomicadd, intrusive_ptr]</li>
<li>C++11</li>
<li>lapack</li>
<li>blas</li>
<li>gcc v4.8.5+ (recommand v6+) (required -std=c++11)</li>
</ul>
<p>[CUDA support]</p><ul>
<li>CUDA v10+</li>
<li>cuDNN</li>
</ul>
<p>[OpenMp support]</p><ul>
<li>openmp</li>
</ul>
<p>[Python]</p><ul>
<li>pybind11 2.2.4</li>
<li>numpy &gt;= 1.15</li>
</ul>
<h2>conda install</h2>
<p>[Currently Linux only]</p>
<p>without CUDA</p><ul>
<li>python 3.6: conda install -c kaihsinwu cytnx_36</li>
<li>python 3.7: conda install -c kaihsinwu cytnx_37</li>
</ul>
<p>with CUDA</p><ul>
<li>python 3.6: conda install -c kaihsinwu cytnx_cuda_36</li>
<li>python 3.7: conda install -c kaihsinwu cytnx_cuda_37</li>
</ul>
<h2>compile</h2>
<p>1.) create a build folder, and cd to the folder $mkdir build</p>
<p>$cd build</p>
<p>2.) resolving the dependency </p><pre class="fragment">$cmake [flags (optional)] &lt;Cytnx repo folder&gt;

[Note] there are several customize flags format as (-D&lt;flag name&gt;).

* USE_ICPC (default = off)

    The default compiler is g++-6. 

* USE_CUDA (default = off)

    If USE_CUDA=1, the code will compile with GPU support.

* USE_MKL (default = off)

    If USE_MKL=off, the code will compile with system LAPACK/BLAS library. 

* CMAKE_INSTALL_PREFIX (default is /usr/local)

    Set the install target path.
</pre><p>3.) compile by running: </p><pre class="fragment">$make -Bj4
</pre><p>4.) install to the target path. </p><pre class="fragment">$make install
</pre><h2>Some snippets:</h2>
<h3>Storage</h3>
<ul>
<li>Memory container with GPU/CPU support. maintain type conversions (type casting btwn Storages) and moving btwn devices.</li>
<li>Generic type object, the behavior is very similar to python. <div class="fragment"><div class="line">Storage A(400,<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i=0;i&lt;400;i++)</div><div class="line">    A.<a class="code" href="classcytnx_1_1Tensor.html#a697b114d1df390ca558ea9211c2d683b">at</a>&lt;<span class="keywordtype">double</span>&gt;(i) = i;</div><div class="line"></div><div class="line">Storage B = A; <span class="comment">// A and B share same memory, this is similar as python </span></div><div class="line"></div><div class="line">Storage C = A.<a class="code" href="classcytnx_1_1Tensor.html#acf7f697a9434f9bc98a7d00a555ee982">to</a>(<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0); </div></div><!-- fragment --></li>
</ul>
<h3>Tensor</h3>
<ul>
<li>A tensor, API very similar to numpy and pytorch.</li>
<li>simple moving btwn CPU and GPU: <div class="fragment"><div class="line">Tensor A({3,4},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double,<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cpu); <span class="comment">// create tensor on CPU (default)</span></div><div class="line">Tensor B({3,4},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double,<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0); <span class="comment">// create tensor on GPU with gpu-id=0</span></div><div class="line"></div><div class="line"></div><div class="line">Tensor C = B; <span class="comment">// C and B share same memory.</span></div><div class="line"></div><div class="line"><span class="comment">// move A to gpu</span></div><div class="line">Tensor D = A.<a class="code" href="classcytnx_1_1Tensor.html#acf7f697a9434f9bc98a7d00a555ee982">to</a>(<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0);</div><div class="line"></div><div class="line"><span class="comment">// inplace move A to gpu</span></div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a114a31fbb8bf4a90f150b6a67e42183a">to_</a>(<a class="code" href="namespacecytnx.html#a0159aceab8996db3faf89716e4dd6b9a">Device</a>.cuda+0);</div></div><!-- fragment --></li>
<li>Type conversion in between avaliable: <div class="fragment"><div class="line">Tensor A({3,4},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line">Tensor B = A.<a class="code" href="classcytnx_1_1Tensor.html#a7b996d3281e7375b29a7cfe4273b299f">astype</a>(<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Uint64); <span class="comment">// cast double to uint64_t</span></div></div><!-- fragment --></li>
<li>vitual swap and permute. All the permute and swap will not change the underlying memory</li>
<li>Use Contiguous() when needed to actual moving the memory layout. <div class="fragment"><div class="line">Tensor A({3,4,5,2},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a4239ec84bd34f634a822eeff416f4d63">permute_</a>({0,3,1,2}); <span class="comment">// this will not change the memory, only the shape info is changed.</span></div><div class="line">cout &lt;&lt; A.<a class="code" href="classcytnx_1_1Tensor.html#a6a1d9ed962b0e9a484e2bb0de15eb76d">is_contiguous</a>() &lt;&lt; endl; <span class="comment">// this will be false!</span></div><div class="line"></div><div class="line">A.<a class="code" href="classcytnx_1_1Tensor.html#a3d4342299e6951dae13b6136b89f1d53">contiguous_</a>(); <span class="comment">// call Configuous() to actually move the memory.</span></div><div class="line">cout &lt;&lt; A.<a class="code" href="classcytnx_1_1Tensor.html#a6a1d9ed962b0e9a484e2bb0de15eb76d">is_contiguous</a>() &lt;&lt; endl; <span class="comment">// this will be true!</span></div></div><!-- fragment --></li>
<li>access single element using .at <div class="fragment"><div class="line">Tensor A({3,4,5},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line"><span class="keywordtype">double</span> val = A.<a class="code" href="classcytnx_1_1Tensor.html#a697b114d1df390ca558ea9211c2d683b">at</a>&lt;<span class="keywordtype">double</span>&gt;({0,2,2});</div></div><!-- fragment --></li>
<li>access elements with python slices similarity: <div class="fragment"><div class="line"><span class="keyword">typedef</span> Accessor <a class="code" href="DenseCyTensor_8cpp.html#abb126f94f3d42e9104cf6bb7d4c035b8">ac</a>;</div><div class="line">Tensor A({3,4,5},<a class="code" href="namespacecytnx.html#ac2c6c045a5fd258e21bd0786744839c2">Type</a>.Double);</div><div class="line">Tensor out = A.<a class="code" href="classcytnx_1_1Tensor.html#ad7b928e4cb89d40cbd99aefab9aa0075">get</a>({<a class="code" href="DenseCyTensor_8cpp.html#abb126f94f3d42e9104cf6bb7d4c035b8">ac</a>(0),<a class="code" href="classcytnx_1_1Accessor.html#a71b8c4af7182a2c9144929bdef9ff4fd">ac::all</a>(),<a class="code" href="classcytnx_1_1Accessor.html#a7a5a508a58b71897c3dd162195aceaa9">ac::range</a>(1,4)}); </div><div class="line"><span class="comment">// equivalent to python: out = A[0,:,1:4]    </span></div></div><!-- fragment --></li>
</ul>
<h2>Fast Examples</h2>
<pre class="fragment">See test.cpp for using C++ .
See test.py for using python  
</pre><h2>Developer</h2>
<pre class="fragment">Kai-Hsin Wu kaihsinwu@gmail.com 
Yen-Hsin Wu
Yu-Hsueh Chen 
</pre><h2>Refereces:</h2>
<pre class="fragment">* example/DMRG:
    https://www.tensors.net/dmrg</pre> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
