%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



\title{Cytnx}
\date{Jun 28, 2020}
\release{}
\author{Kai\sphinxhyphen{}Hsin Wu}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}
\noindent\sphinxincludegraphics[width=350\sphinxpxdimen]{{Icon_small}.png}


\begin{quote}

Cytnx is a library design for Quantum physics simulation using GPUs and CPUs.
\end{quote}


\chapter{Introdoction}
\label{\detokenize{Intro:introdoction}}\label{\detokenize{Intro::doc}}\begin{quote}

Cytnx is a library design for Quantum/classical Physics simulation.

The library is build from bottum\sphinxhyphen{}up, with both C++ and python in mind right at the beginning of developement. That’s why nearly 95\% of the APIs are exactly the same in both C++ and python ends.

Most of Cytnx APIs share very similar interfaces as the most common and popular libraries: numpy/scipy/pytorch. This is spcifically designed so as to reduce the learning curve for users. Further more, we implement these easy\sphinxhyphen{}to\sphinxhyphen{}use python libraries interface to C++ side in hope to benefit users who want to bring their python programming experience to C++ side and speed up their programs.

Cytnx also support multi\sphinxhyphen{}devices (CPUs/GPUs) directly in the base container level. Especially, not only the container but also our linear algebra fucntions share the same APIs regadless of the devices where the input Tensors are, just like pytorch. This provides users ability to accelerate the code without worrying too much details about multi\sphinxhyphen{}devices programming.

From the physics side, cytnx\_extension namespace/submodule provides powerful tools such as CyTensor, Network, Bond, Symmetry etc. These objects are built on top of Tensor objects, spcifically aiming for reduce the developing work of Tensor network algorithm by simplify the user interfaces.

In this user guide, both python and C++ APIs will be discussed, provided side\sphinxhyphen{}by\sphinxhyphen{}side for users to better understand how to use Cytnx, and understand the conversion in between Python API and C++ API.
\end{quote}


\section{Features}
\label{\detokenize{Intro:features}}\begin{itemize}
\item {} 
C++ and python are co\sphinxhyphen{}exists, there is no one first.

\item {} 
95\% of API are the same in C++ and Python.
This means one can do a fast prototype in python, and directly convert to C++ with extremely minimal re\sphinxhyphen{}writing of codebase.

\item {} 
GPUs/CPUs multi\sphinxhyphen{}devices support.

\item {} 
Easy to use user\sphinxhyphen{}interface similar to numpy/scipy/pytorch.

\item {} 
Enhance tools specifically designs for Quantum/classical Physics simulation.

\end{itemize}


\chapter{User Guide}
\label{\detokenize{Guide:user-guide}}\label{\detokenize{Guide::doc}}
To use the library, simply include/import cytnx.

In Python, using import

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{cytnx}
\end{sphinxVerbatim}

In C++, using include header

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}cytnx.hpp\PYGZdq{};}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
In C++, there is a namespace \sphinxstylestrong{cytnx}.
\end{sphinxadmonition}

There are equivalence of alias between python module and c++ namespace, for example if we want to alias cytnx as cy,

In python :

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{cytnx} \PYG{k}{as} \PYG{n+nn}{cy}
\end{sphinxVerbatim}

This is equivalent in C++ as:

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}cytnx.hpp\PYGZdq{};}
\PYG{k}{namespace} \PYG{n}{cy}\PYG{o}{=}\PYG{n}{cytnx}\PYG{p}{;}
\end{sphinxVerbatim}

\sphinxstylestrong{Now we are ready to start using cytnx!}

Continue reading:


\section{Basic objects}
\label{\detokenize{guide/basic_obj:basic-objects}}\label{\detokenize{guide/basic_obj::doc}}

\subsection{Tensor}
\label{\detokenize{guide/basic_obj/Tensor:tensor}}\label{\detokenize{guide/basic_obj/Tensor::doc}}
Tensor is the basic building block of Cytnx.
In fact, the API of Tensor in cytnx is very similar to \sphinxhref{https://pytorch.org/docs/stable/tensors.html}{torch.tensor} (so as numpy.array, since they are also similar to each other)

Let’s take a look on how to use it:


\subsubsection{1. Create a Tensor}
\label{\detokenize{guide/basic_obj/Tensor_1_creat:create-a-tensor}}\label{\detokenize{guide/basic_obj/Tensor_1_creat::doc}}
Just like \sphinxhref{https://numpy.org/doc/1.18/reference/generated/numpy.array.html}{numpy.array} / \sphinxhref{https://pytorch.org/docs/stable/tensors.html}{torch.tensor}, Tensor is generally created using generator such as \sphinxstylestrong{zero()}, \sphinxstylestrong{arange()}, \sphinxstylestrong{ones()}.

For example, suppose we want to define a rank\sphinxhyphen{}3 tensor with shape (3,4,5), and initialize all elements with zero:
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Tensor} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
In cytnx, the conversion of python list is equivalent to C++ \sphinxstyleemphasis{vector}; or in some case like here, it is a \sphinxstyleemphasis{initializer list}.

\item {} 
The conversion in between is pretty straight forward, one simply replace {[}{]} in python with \{\}, and you are all set!

\end{enumerate}
\end{sphinxadmonition}

Other options such as \sphinxstylestrong{arange()} (similar as np.arange), and \sphinxstylestrong{ones} (similar as np.ones) can also be done.
\begin{itemize}
\item {} 
In python :

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{;}     \PYG{c+c1}{\PYGZsh{}rank\PYGZhy{}1 Tensor from [0,10) with step 1}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{\PYGZsh{}rank\PYGZhy{}1 Tensor from [0,10) with step 2}
\PYG{n}{C} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}  \PYG{c+c1}{\PYGZsh{}Tensor of shape (3,4,5) with all elements set to one.}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{;}     \PYG{c+c1}{//rank\PYGZhy{}1 Tensor from [0,10) with step 1}
\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{//rank\PYGZhy{}1 Tensor from [0,10) with step 2}
\PYG{k}{auto} \PYG{n}{C} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}  \PYG{c+c1}{//Tensor of shape (3,4,5) with all elements set to one.}
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{Tips}] \leavevmode
In C++, you could make use of \sphinxstyleemphasis{auto} to simplify your code!

\end{description}\end{quote}


\paragraph{1.1. Tensor with different dtype and device}
\label{\detokenize{guide/basic_obj/Tensor_1_creat:tensor-with-different-dtype-and-device}}
By default, the Tensor will be created with \sphinxstyleemphasis{double} type (or \sphinxstyleemphasis{float} in python) on CPU if there is no additional arguments provided upon creating the Tensor.

You can create a Tensor with different data type, and/or on different devices simply by specify the \sphinxstylestrong{dtype} and the \sphinxstylestrong{device} arguments upon initialization. For example, the following codes create a Tensor with 64bit integer on cuda\sphinxhyphen{}enabled GPU.
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,}\PYG{n}{dtype}\PYG{o}{=}\PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{Type}\PYG{o}{.}\PYG{n}{Int64}\PYG{p}{,}\PYG{n}{device}\PYG{o}{=}\PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{Device}\PYG{o}{.}\PYG{n}{cuda}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{p}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Type}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Int64}\PYG{p}{,}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Device}\PYG{o}{:}\PYG{o}{:}\PYG{n}{cuda}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Remember the difference of . in python and :: in C++ when you use Type and Device classes.

\item {} 
If you have multiple GPUs, you can specify which GPU you want to init Tensor by adding gpu\sphinxhyphen{}id to cytnx::Device::cuda.
\begin{quote}

For example:
\begin{quote}

device=cytnx.Device.cuda+2   \#will create Tensor on GPU id=2

device=cytnx.Device.cuda+4   \#will create Tensor on GPU id=4
\end{quote}
\end{quote}

\item {} 
In C++, there is no keyword argument as python, so make sure you put the argument in the correct order. Check \sphinxhref{https://kaihsin.github.io/Cytnx/docs/html/index.html}{API documentation} for function signatures!

\end{enumerate}
\end{sphinxadmonition}

Currently, there are several data types supported by cytnx:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|l|l|l|}
\hline
\sphinxstyletheadfamily 
cytnx type
&\sphinxstyletheadfamily 
c++ type
&\sphinxstyletheadfamily 
Type object
\\
\hline
cytnx\_double
&
double
&
Type.Double
\\
\hline
cytnx\_float
&
float
&
Type.Float
\\
\hline
cytnx\_uint64
&
uint64\_t
&
Type.Uint64
\\
\hline
cytnx\_uint32
&
uint32\_t
&
Type.Uint32
\\
\hline
cytnx\_uint16
&
uint16\_t
&
Type.Uint16
\\
\hline
cytnx\_int64
&
int64\_t
&
Type.Int64
\\
\hline
cytnx\_int32
&
int32\_t
&
Type.Int32
\\
\hline
cytnx\_int16
&
int16\_t
&
Type.Int16
\\
\hline
cytnx\_complex128
&
std::complex\textless{}double\textgreater{}
&
Type.ComplexDouble
\\
\hline
cytnx\_complex64
&
std::complex\textless{}float\textgreater{}
&
Type.ComplexFloat
\\
\hline
cytnx\_bool
&
bool
&
Type.Bool
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

For devices, Cytnx currently supports


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|l|l|}
\hline
\sphinxstyletheadfamily 
cytnx type
&\sphinxstyletheadfamily 
Device object
\\
\hline
CPU
&
Device.cpu
\\
\hline
CUDA\sphinxhyphen{}enabled GPU
&
Device.cuda+x
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\paragraph{1.2 Type conversion}
\label{\detokenize{guide/basic_obj/Tensor_1_creat:type-conversion}}
It is possible to convert a Tensor to a different data type. To convert the data type, simply use \sphinxstylestrong{Tensor.astype()}.

For example, consider a Tensor \sphinxstyleemphasis{A} with \sphinxstylestrong{dtype=Type.Int64}, and we want to convert it to \sphinxstylestrong{Type.Double}
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,}\PYG{n}{dtype}\PYG{o}{=}\PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{Type}\PYG{o}{.}\PYG{n}{Int64}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{Type}\PYG{o}{.}\PYG{n}{Double}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{dtype\PYGZus{}str}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{B}\PYG{o}{.}\PYG{n}{dtype\PYGZus{}str}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Type}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Int64}\PYG{p}{)}\PYG{p}{;}
\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Type}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Double}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A}\PYG{p}{.}\PYG{n}{dtype\PYGZus{}str}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{B}\PYG{p}{.}\PYG{n}{dtype\PYGZus{}str}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

\textgreater{}\textgreater{} Output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Int64
Double (Float64)
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Use Tensor.dtype() will return a type\sphinxhyphen{}id, where Tensor.dtype\_str() will return the type name.

\item {} 
Complex data type cannot directly convert to real data type. Use Tensor.real()/Tensor.imag() if you want to get the real/imag part.

\end{enumerate}
\end{sphinxadmonition}


\paragraph{1.3 Transfer btwn devices}
\label{\detokenize{guide/basic_obj/Tensor_1_creat:transfer-btwn-devices}}
To move a Tensor between different devices is very easy. We can use \sphinxstylestrong{Tensor.to()} to move the Tensor to a different device.

For example, let’s create a Tensor on cpu and transfer to GPU with gpu\sphinxhyphen{}id=0.
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}on CPU}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{A}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{Device}\PYG{o}{.}\PYG{n}{cuda}\PYG{o}{+}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{//on CPU}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{A}\PYG{p}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{cytnx}\PYG{p}{.}\PYG{n}{Device}\PYG{p}{.}\PYG{n}{cuda}\PYG{o}{+}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

\textgreater{}\textgreater{} Output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 4
type  : Double (Float64)
cytnx device: CPU
Shape : (2,2)
[[1.00000e+00 1.00000e+00 ]
 [1.00000e+00 1.00000e+00 ]]

Total elem: 4
type  : Double (Float64)
cytnx device: CUDA/GPU\PYGZhy{}id:0
Shape : (2,2)
[[1.00000e+00 1.00000e+00 ]
 [1.00000e+00 1.00000e+00 ]]
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
You can use \sphinxstylestrong{Tensor.device()} to get the current device\sphinxhyphen{}id (cpu = \sphinxhyphen{}1), where as \sphinxstylestrong{Tensor.device\_str()} returns the device name.
\end{sphinxadmonition}


\subsubsection{2. Manipulate Tensor}
\label{\detokenize{guide/basic_obj/Tensor_2_manip:manipulate-tensor}}\label{\detokenize{guide/basic_obj/Tensor_2_manip::doc}}
Next, let’s look at the operations that are commonly used to manipulate Tensor object.


\paragraph{2.1 reshape}
\label{\detokenize{guide/basic_obj/Tensor_2_manip:reshape}}
Suppose we want to create a rank\sphinxhyphen{}3 Tensor with shape=(2,3,4), starting with a rank\sphinxhyphen{}1 Tensor with shape=(24) initialized using \sphinxstylestrong{arange()}.

This operation is called \sphinxstyleemphasis{reshape}

We can use \sphinxstylestrong{Tensor.reshape} function to do this.
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In C++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{;}
\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{B} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

\textgreater{}\textgreater{} Output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (24)
[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]

Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]
\end{sphinxVerbatim}

Notice that calling \sphinxstylestrong{reshape()} returns a new object \sphinxstyleemphasis{B}, so the original object \sphinxstyleemphasis{A} is not changed after calls reshape.

There is the other function \sphinxstylestrong{Tensor.reshape\_} (with a underscore) that also performs reshape, but instead of return a new reshaped object, it performs inplace reshape to the instance that calls the function. For example:
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n}{A}\PYG{o}{.}\PYG{n}{reshape\PYGZus{}}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In C++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{A}\PYG{p}{.}\PYG{n}{reshape\PYGZus{}}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

\textgreater{}\textgreater{} Output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (24)
[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]

Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]
\end{sphinxVerbatim}

Thus we see that using underscore version modify the instance itself.

\begin{sphinxadmonition}{note}{Note:}
In general, all the funcions in Cytnx that end with a underscore \_ is either a inplace function that modify the instance that calls it, or return the reference of some class member.
\end{sphinxadmonition}

\begin{sphinxadmonition}{hint}{Hint:}
You can use \sphinxstylestrong{Tensor.shape()} to get the shape of Tensor.
\end{sphinxadmonition}


\paragraph{2.1 permute}
\label{\detokenize{guide/basic_obj/Tensor_2_manip:permute}}
Now, let’s again use the same rank\sphinxhyphen{}3  with shape=(2,3,4) as example. This time we want to do permute on the Tensor to exchange axes from indices (0,1,2)\sphinxhyphen{}\textgreater{}(1,2,0)

This can be achieved with \sphinxstylestrong{Tensor.permute}
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{o}{.}\PYG{n}{permute}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{.}\PYG{n}{permute}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{B} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

\textgreater{}\textgreater{} Output:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]

Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (3,4,2)
[[[0.00000e+00 1.20000e+01 ]
  [1.00000e+00 1.30000e+01 ]
  [2.00000e+00 1.40000e+01 ]
  [3.00000e+00 1.50000e+01 ]]
 [[4.00000e+00 1.60000e+01 ]
  [5.00000e+00 1.70000e+01 ]
  [6.00000e+00 1.80000e+01 ]
  [7.00000e+00 1.90000e+01 ]]
 [[8.00000e+00 2.00000e+01 ]
  [9.00000e+00 2.10000e+01 ]
  [1.00000e+01 2.20000e+01 ]
  [1.10000e+01 2.30000e+01 ]]]
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
Just like before, there is an equivalent \sphinxstylestrong{Tensor.permute\_} end with underscore that performs inplace permute on the instance that calls it.
\end{sphinxadmonition}

\begin{sphinxadmonition}{hint}{Hint:}
In some situation where we don’t want to create a copy of object, using inplace version of functions can reduce the memory usage.
\end{sphinxadmonition}

In Cytnx, the permute operation does not moving the elements in the memory immediately. Only the meta\sphinxhyphen{}data that is seen by user are changed.
This can avoid the redudant moving of elements. Note that this approach is also taken in \sphinxhref{https://numpy.org/doc/1.18/reference/generated/numpy.array.html}{numpy.array} and \sphinxhref{https://pytorch.org/docs/stable/tensors.html}{torch.tensor} .

If the meta\sphinxhyphen{}data is distached from the real memery layout, we call the Tensor in this status \sphinxstyleemphasis{non\sphinxhyphen{}contiguous}. We can use \sphinxstylestrong{Tensor.is\_contiguous()} to check if the current Tensor is in contiguous status.

You can force the Tensor to return to it’s contiguous status by calling \sphinxstylestrong{Tensor.contiguous()/Tensor.contiguous\_()}, although generally you don’t have to worry about contiguous, as cytnx automatically handles it for you.
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{n}{A}\PYG{o}{.}\PYG{n}{permute\PYGZus{}}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{n}{A}\PYG{o}{.}\PYG{n}{contiguous\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{o}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In C++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A}\PYG{p}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

\PYG{n}{A}\PYG{p}{.}\PYG{n}{permute\PYGZus{}}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A}\PYG{p}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

\PYG{n}{A}\PYG{p}{.}\PYG{n}{contiguous\PYGZus{}}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A}\PYG{p}{.}\PYG{n}{is\PYGZus{}contiguous}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

Output\textgreater{}\textgreater{}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
True

Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]

False

Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (3,2,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]]
 [[4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]]
 [[8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]

True
\end{sphinxVerbatim}

\begin{sphinxadmonition}{tip}{Tip:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Generally, you don’t have to worry about contiguous issue. you can access the elements and call linalg just like this contiguous/non\sphinxhyphen{}contiguous thing doesn’t exist.

\item {} 
In the case where the function does require user to manually make the Tensor contiguous, a warning will be prompt, and you can simply add a \sphinxstylestrong{Tensor.contiguous()/.contiguous\_()} before the function call.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
As metioned before, \sphinxstylestrong{Tensor.contiguous\_()} (with underscore) make the current instance contiguous, while \sphinxstylestrong{Tensor.contiguous()} return a new object with contiguous status.
In the case where the current instance is already in it’s contiguous status, calling contiguous will return itself, and no new object will be created.
\end{sphinxadmonition}


\subsubsection{3. Access elements}
\label{\detokenize{guide/basic_obj/Tensor_3_access:access-elements}}\label{\detokenize{guide/basic_obj/Tensor_3_access::doc}}
Next, let’s take a look on how we can access elements inside a Tensor.


\paragraph{3.1 Get elements}
\label{\detokenize{guide/basic_obj/Tensor_3_access:get-elements}}
Just like python list/numpy.array/torch.tensor, on the python side, we can simply use \sphinxstyleemphasis{slice} to get the elements. See \sphinxhref{https://numpy.org/doc/stable/reference/arrays.indexing.html}{This page} .
In c++, cytnx take this approach from python and bring it to our C++ API. You can simply use the \sphinxstylestrong{slice string} to access elements.

For example:
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{l+m+mi}{4}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}

\PYG{n}{C} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{C}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In C++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{:}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{1:4:2}\PYG{l+s}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{B} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

\PYG{k}{auto} \PYG{n}{C} \PYG{o}{=} \PYG{n}{A}\PYG{p}{(}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{:}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{C} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

Output\textgreater{}\textgreater{}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]


Total elem: 6
type  : Double (Float64)
cytnx device: CPU
Shape : (3,2)
[[1.00000e+00 3.00000e+00 ]
 [5.00000e+00 7.00000e+00 ]
 [9.00000e+00 1.10000e+01 ]]


Total elem: 8
type  : Double (Float64)
cytnx device: CPU
Shape : (2,4)
[[4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
 [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]]
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
To convert in between python and C++ APIs, notice that in C++, we use operator() instead of operator{[}{]} if you are using slice string to acess elements.

\item {} 
The return will always be Tensor object, even it is only one elements in the Tensor.

\end{enumerate}
\end{sphinxadmonition}

In the case where you have only one element in a Tensor, we can use \sphinxstylestrong{item()} to get the element in the standard python type/c++ type.
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{C} \PYG{o}{=} \PYG{n}{B}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{C}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In C++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{A}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{double} \PYG{n}{C} \PYG{o}{=} \PYG{n}{B}\PYG{p}{.}\PYG{n}{item}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{double}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{B} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{C} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

Output\textgreater{}\textgreater{}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 1
type  : Double (Float64)
cytnx device: CPU
Shape : (1)
[1.00000e+00 ]


1.0
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
In C++, using \sphinxstylestrong{item\textless{}\textgreater{}()} to get the element require explicitly specify the type that match the dtype of the Tensor. If the type specify does not match, an error will be prompt.
\end{sphinxadmonition}


\paragraph{3.2 Set elememts}
\label{\detokenize{guide/basic_obj/Tensor_3_access:set-elememts}}
Setting elements is pretty much the same as numpy.array/torch.tensor. You can assign a Tensor to a specific slice, our set all the elements in that slice to be the same value.

For example:
\begin{itemize}
\item {} 
In python:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{B}\PYG{p}{)}

\PYG{n}{A}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{p}{:}\PYG{p}{,}\PYG{p}{:}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{=} \PYG{n}{B}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}

\PYG{n}{A}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{p}{:}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{4}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{A}\PYG{p}{)}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
In c++:

\end{itemize}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
\PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{B} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

\PYG{n}{A}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{:}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{::2}\PYG{l+s}{\PYGZdq{}}\PYG{p}{)} \PYG{o}{=} \PYG{n}{B}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

\PYG{n}{A}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+s}{\PYGZdq{}}\PYG{l+s}{::2}\PYG{l+s}{\PYGZdq{}}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{4}\PYG{p}{;}
\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{A} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{sphinxVerbatim}

Output\textgreater{}\textgreater{}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[1.20000e+01 1.30000e+01 1.40000e+01 1.50000e+01 ]
  [1.60000e+01 1.70000e+01 1.80000e+01 1.90000e+01 ]
  [2.00000e+01 2.10000e+01 2.20000e+01 2.30000e+01 ]]]


Total elem: 6
type  : Double (Float64)
cytnx device: CPU
Shape : (3,2)
[[0.00000e+00 0.00000e+00 ]
 [0.00000e+00 0.00000e+00 ]
 [0.00000e+00 0.00000e+00 ]]


Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 2.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 1.00000e+01 1.10000e+01 ]]
 [[0.00000e+00 1.30000e+01 0.00000e+00 1.50000e+01 ]
  [0.00000e+00 1.70000e+01 0.00000e+00 1.90000e+01 ]
  [0.00000e+00 2.10000e+01 0.00000e+00 2.30000e+01 ]]]


Total elem: 24
type  : Double (Float64)
cytnx device: CPU
Shape : (2,3,4)
[[[0.00000e+00 1.00000e+00 4.00000e+00 3.00000e+00 ]
  [4.00000e+00 5.00000e+00 6.00000e+00 7.00000e+00 ]
  [8.00000e+00 9.00000e+00 4.00000e+00 1.10000e+01 ]]
 [[0.00000e+00 1.30000e+01 0.00000e+00 1.50000e+01 ]
  [0.00000e+00 1.70000e+01 0.00000e+00 1.90000e+01 ]
  [0.00000e+00 2.10000e+01 0.00000e+00 2.30000e+01 ]]]
\end{sphinxVerbatim}


\paragraph{3.3 Low\sphinxhyphen{}level API (C++ only)}
\label{\detokenize{guide/basic_obj/Tensor_3_access:low-level-api-c-only}}
On C++ side, cytnx provide lower\sphinxhyphen{}level APIs with slightly smaller overhead for getting elements.
These low\sphinxhyphen{}level APIs require using with \sphinxstylestrong{Accessor} object.
\begin{itemize}
\item {} \begin{description}
\item[{Accessor:}] \leavevmode
\sphinxstylestrong{Accessor} object is equivalent to python \sphinxstyleemphasis{slice}. It is sometimes convenient to use alias to simplify the expression when using it.

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
    \PYG{k}{typedef} \PYG{n}{ac}\PYG{o}{=}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Accessor}\PYG{p}{;}

    \PYG{n}{ac}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}     \PYG{c+c1}{// this equal to index \PYGZsq{}4\PYGZsq{} in python}
    \PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{// this equal to \PYGZsq{}:\PYGZsq{} in python}
    \PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{// this equal to \PYGZsq{}0:4:2\PYGZsq{} in python}
\end{sphinxVerbatim}

\end{description}

\end{itemize}

In the following, let’s see how it can be used to get/set the elements from/to Tensor.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
operator{[}{]} (middle level API) :

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
    \PYG{k}{typedef} \PYG{n}{ac}\PYG{o}{=}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Accessor}\PYG{p}{;}
    \PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// [get] this is equal to A[0,:,1:4:2] in python:}
    \PYG{k}{auto} \PYG{n}{C} \PYG{o}{=} \PYG{n}{A}\PYG{p}{[}\PYG{p}{\PYGZob{}}\PYG{n}{ac}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{]}\PYG{p}{;}

    \PYG{c+c1}{// [set] this is equal to A[1,:,0:4:2] = B in python:}
    \PYG{n}{A}\PYG{p}{[}\PYG{p}{\PYGZob{}}\PYG{n}{ac}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{B}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
Remember to put a braket\{\}. This because C++ operator{[}{]} can only accept one argument.
\end{sphinxadmonition}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} \begin{description}
\item[{get/set (lowest level API) :}] \leavevmode
get() and set() is the lowest\sphinxhyphen{}level API. Operator() and Operator{[}{]} are all build base on these.

\end{description}

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
    \PYG{k}{typedef} \PYG{n}{ac}\PYG{o}{=}\PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Accessor}\PYG{p}{;}
    \PYG{k}{auto} \PYG{n}{A} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{auto} \PYG{n}{B} \PYG{o}{=} \PYG{n}{cytnx}\PYG{o}{:}\PYG{o}{:}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// [get] this is equal to A[0,:,1:4:2] in python:}
    \PYG{k}{auto} \PYG{n}{C} \PYG{o}{=} \PYG{n}{A}\PYG{p}{.}\PYG{n}{get}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{ac}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// [set] this is equal to A[1,:,0:4:2] = B in python:}
    \PYG{n}{A}\PYG{p}{.}\PYG{n}{set}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{ac}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{all}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{ac}\PYG{o}{:}\PYG{o}{:}\PYG{n}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{B}\PYG{p}{)}\PYG{p}{;}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{hint}{Hint:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
Similarly, you can also pass a c++ \sphinxstyleemphasis{vector\textless{}cytnx\_int64\textgreater{}} as argument.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{tip}{Tip:}
If your code requires frequently get/set elements, using low\sphinxhyphen{}level API can reduce the overhead.
\end{sphinxadmonition}


\subsubsection{4. Tensor arithmetic}
\label{\detokenize{guide/basic_obj/Tensor_4_arithmetic:tensor-arithmetic}}\label{\detokenize{guide/basic_obj/Tensor_4_arithmetic::doc}}
In cytnx, Tensor can performs arithmetic operation such as \sphinxstylestrong{+, \sphinxhyphen{}, x, /, +=, \sphinxhyphen{}=, *=, /=} with another Tensor or scalalr, just like the standard way you do in python.


\paragraph{4.1 Type promotion}
\label{\detokenize{guide/basic_obj/Tensor_4_arithmetic:type-promotion}}
Arithmetic operation in Cytnx follows the similar pattern of type promotion as standard C++/python.
When Tensor performs arithmetic operation with another Tensor or scalar, the output Tensor will have the dtype as the one that has stronger type.

The Types order from strong to weak as:
\begin{itemize}
\item {} 
Type.ComplexDouble

\item {} 
Type.ComplexFloat

\item {} 
Type.Double

\item {} 
Type.Float

\item {} 
Type.Int64

\item {} 
Type.Uint64

\item {} 
Type.Int32

\item {} 
Type.Uint32

\item {} 
Type.Int16

\item {} 
Type.Uint16

\item {} 
Type.Bool

\end{itemize}


\paragraph{4.2 Tensor\sphinxhyphen{}Tensor arithmetic}
\label{\detokenize{guide/basic_obj/Tensor_4_arithmetic:tensor-tensor-arithmetic}}
Tensor can performs arithmetic operation with another Tensor with the same shape.


\paragraph{4.3 Tensor\sphinxhyphen{}scalar arithmetic}
\label{\detokenize{guide/basic_obj/Tensor_4_arithmetic:tensor-scalar-arithmetic}}
Tensor can also performs arithmetic operation with scalar.


\paragraph{4.4 Equivalent APIs}
\label{\detokenize{guide/basic_obj/Tensor_4_arithmetic:equivalent-apis}}
Following are some equivalent APIs that  are also provided in Cytnx for users who are familiar and coming from pytorch and other librariy communities.

\begin{sphinxadmonition}{note}{Note:}
1. All the arithmetic operation function such as \sphinxstylestrong{Add,Sub,Mul,Div…}, as well as linear algebra functions all start with capital characters.
While in pytorch, they are all lower\sphinxhyphen{}case.
2. All the arithmetic operations with a underscore (such as \sphinxstylestrong{Add\_, Sub\_, Mul\_, Div\_})are the inplace version that modify the current instance.
\end{sphinxadmonition}

\begin{sphinxadmonition}{hint}{Hint:}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
ComplexDouble/ComplexFloat/Double/Float, these 4 types internally calls BLAS/cuBLAS/MKL ?axpy when the inputs are in the same types.

\item {} 
Arithmetic between other types (Including different types) are accelerated with OpenMP on CPU. For GPU, custom kernels are used to perform operation.

\end{enumerate}
\end{sphinxadmonition}


\section{Linear algebra}
\label{\detokenize{guide/linalg:linear-algebra}}\label{\detokenize{guide/linalg::doc}}

\section{Iterative solver}
\label{\detokenize{guide/itersol:iterative-solver}}\label{\detokenize{guide/itersol::doc}}

\section{Cytnx extensions}
\label{\detokenize{guide/cyx:cytnx-extensions}}\label{\detokenize{guide/cyx::doc}}

\section{linalg extension}
\label{\detokenize{guide/xlinalg:linalg-extension}}\label{\detokenize{guide/xlinalg::doc}}

\chapter{Examples}
\label{\detokenize{Examples:examples}}\label{\detokenize{Examples::doc}}
Here, we provide some examples of commonly seen Quantum simualtion methods.


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}