
\begin{DoxyCodeInclude}{0}
\DoxyCodeLine{v0.6.5}
\DoxyCodeLine{1. [Fix] Bug in UniTensor \_Load    }
\DoxyCodeLine{2. [Enhance] Improve stability in Lanczos\_ER  }
\DoxyCodeLine{3. [Enhance] Move \_SII to stack.}
\DoxyCodeLine{4. [Enhance] Add LinOp operator() for mv\_elem}
\DoxyCodeLine{5. [Enhance] Add c++ API fast link to cutt}
\DoxyCodeLine{6. [Enhance] Add Fromfile/Tofile for load/save binary files @ Tensor/Storage}
\DoxyCodeLine{7. [Enhance] Add linspace generator}
\DoxyCodeLine{8. [Fix] Bug in Div for fast Blas call bug}
\DoxyCodeLine{9. [Enhance] Add Tensor.append(Storage) if Tensor is rank-\/2 and dimension match.}
\DoxyCodeLine{10. [Enhance] Add algo namespace}
\DoxyCodeLine{11. [Enhance] Add Sort-\/@cpu}
\DoxyCodeLine{12. [Enhance] add Storage.numpy() for pythonAPI}
\DoxyCodeLine{13. [Enhance] add Tensor.from\_storage() for python API}
\DoxyCodeLine{}
\DoxyCodeLine{v0.6.4}
\DoxyCodeLine{1. [Enhance] Add option mv\_elem for Tensordot, which actually move elements in input tensor. This is beneficial when same tensordot is called multiple times.}
\DoxyCodeLine{2. [Enhance] Add option cacheL, cacheR to Contract of unitensor. which mv the elements of input tensors to the matmul handy position. }
\DoxyCodeLine{3. [Enhance] optimize Network contraction policy to reduce contiguous permute, with is\_clone argument when PutUniTensor.}
\DoxyCodeLine{4. [Enhance] Add Lanczos\_Gnd for fast get ground state and it's eigen value (currently only real float). }
\DoxyCodeLine{5. [Enhance] Add Tridiag python API, and option is\_row}
\DoxyCodeLine{6. [Enhance] C++ API storage add .back<>() function. }
\DoxyCodeLine{7. [Enhance] C++ API storage fix from\_vector() for bool type. }
\DoxyCodeLine{8. [Enhance] Change Network Launch optimal=True behavior. if user order is given, optimal will not have effect.   }
\DoxyCodeLine{9. [Enhance] Add example/iDMRG/dmrg\_optim.py for better performace with Lanczos\_Gnd and Network cache.}
\DoxyCodeLine{10. [Fix] wrong error message in linalg::Cpr}
\DoxyCodeLine{11. [Fix] reshape() on a already contiguous Tensor will resulting as the change in original tensor, which should not happened.}
\DoxyCodeLine{}
\DoxyCodeLine{v0.6.3}
\DoxyCodeLine{1. [Enhance] Add Device.Ncpus for detecting avaliable omp threads}
\DoxyCodeLine{2. [Enhance] Add HPTT support on CPU permute.}
\DoxyCodeLine{3. [Internal] Build version centralize}
\DoxyCodeLine{4. [Enhance] More info for Device. }
\DoxyCodeLine{6. [Enhance] Add cytnx.\_\_variant\_info\_\_ for checking the installed variant.}
\DoxyCodeLine{}
\DoxyCodeLine{v0.6.2}
\DoxyCodeLine{1. [Fix] Bug in CUDA Matmul interface passing the wrong object bug.  }
\DoxyCodeLine{2. [Enhance] Add Matmul\_dg for diagonal matrix mutiply dense matrix. }
\DoxyCodeLine{3. [Enhance] Add Tensordot\_dg for tensordot with either Tl or Tr is diagonal matrix}
\DoxyCodeLine{4. [Enhance] Contract dense \& sparse memory optimized.      }
\DoxyCodeLine{5. [example] Add iTEBD\_gpu.py example}
\DoxyCodeLine{6. [Fix] Bug in CUDA cuVectordot d and f seg fault }
\DoxyCodeLine{7. [Enhance] Add cuReduce for optimized reduction. }
\DoxyCodeLine{8. [Enhance] Optimize performance for Mul\_internal\_cpu. }
\DoxyCodeLine{9. [Enhance] Optimize performance for Div\_internal\_cpu. }
\DoxyCodeLine{10. [Fix] Bug in permute of UniTensor/Tensor with duplicate entries does not return error.}
\DoxyCodeLine{}
\DoxyCodeLine{v0.6.1}
\DoxyCodeLine{1. [Enhance] add Scalar class (shadow)}
\DoxyCodeLine{2. [Enhance] change default allocation from Malloc to Calloc.}
\DoxyCodeLine{3. [Enhance] change storage.raw\_ptr() to storage.data() and storage.data<>() }
\DoxyCodeLine{4. [Enhance] change storage.cap to STORAGE\_DEFT\_SZ that can be tune.}
\DoxyCodeLine{5. [Enhance] adding Tproxy/Tproxy, Tproxy/Tensor, Tensor/Tproxy operation }
\DoxyCodeLine{6. [Enhance] Add mv\_elem type for LinOp, which intrinsically omp the matvec operation.    }
\DoxyCodeLine{7. [Fatal  ] Fix bug in Dot for Matrix-\/Vector multiplication on both GPU and CPU with complex\&real float dtypes.}
\DoxyCodeLine{}
\DoxyCodeLine{v0.6.0}
\DoxyCodeLine{1. [Enhance] behavior change the behavior of permute to prevent redundant copy in UniTensor and Tensor.}
\DoxyCodeLine{2. add Tensor::same\_data to check if two Tensor has same storage.}
\DoxyCodeLine{3. [Enhance] the behavior of reshape in Tensor to prevent redundant copy.    }
\DoxyCodeLine{4. [Enhance] behavior change all linalg to follow the same disipline for permute/reshape/contiguous}
\DoxyCodeLine{5. [Enhance] add print() in C++ API    }
\DoxyCodeLine{6. [Fix] reshape() does not share memory}
\DoxyCodeLine{7. [Fix] BoolStorage print\_elem does not show the first element in shape}
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.6a}
\DoxyCodeLine{1. [Enhance] change linalg::QR -\/> linalg::Qr for unify the function call }
\DoxyCodeLine{2. Fix bug in UniTensor Qr, R UniTensor labels bug.}
\DoxyCodeLine{3. Add Qdr for UniTensor and Tensor.}
\DoxyCodeLine{4. Fix minor bug in internal, Type.is\_float for Uint32.}
\DoxyCodeLine{5. [Enhance] accessor can now specify with vector. }
\DoxyCodeLine{6. [Enhance] Tproxy.item()}
\DoxyCodeLine{7. Fix inplace reshape\_() in new way templ. does not perform inplace operation}
\DoxyCodeLine{8. [Enhance] Tproxy operator+-\//*}
\DoxyCodeLine{10. Fix bug in division dti64 becomes subtraction bug. }
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.5a}
\DoxyCodeLine{1. [Feature] Tensor can now using operator() to access elements just like python. }
\DoxyCodeLine{2. [Enhance] Access Tensor can now exactly the same using slice string as in python.}
\DoxyCodeLine{3. [Enhance] at/reshape/permute in Tensor can now give args without braket\{\} as in python.}
\DoxyCodeLine{4. [Enhance] Storage.Load to static, so it can match Tensor}
\DoxyCodeLine{5. [Major] Remove cytnx\_extension class, rename CyTensor-\/>UniTensor }
\DoxyCodeLine{6. Fix small bug in return ref of Tproxy }
\DoxyCodeLine{7. Fix bug in buffer size allocation in Svd\_internal }
\DoxyCodeLine{   }
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.4a-\/build1}
\DoxyCodeLine{1. [Important] Fix Subtraction real -\/ complex bug.}
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.4a}
\DoxyCodeLine{1. Add linalg::Det }
\DoxyCodeLine{2. Add Type.is\_float}
\DoxyCodeLine{3. [Feature] Add LinOp class for custom linear operators used in iterative solver}
\DoxyCodeLine{4. enhance arithmetic with scalar Tensors}
\DoxyCodeLine{5. Add Tensor append with tensor. }
\DoxyCodeLine{6. [Feature] Add iterative solver Lanczos\_ER}
\DoxyCodeLine{7. [Enhance] Tproxy +=,-\/=,/=,*= on C++ side}
\DoxyCodeLine{8. Add ED (using Lanczos) example.}
\DoxyCodeLine{9. Change backend to mkl\_ilp64, w/o mkl: OpenBLAS}
\DoxyCodeLine{10. Change Rowrank-\/>rowrank for CyTensor. }
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.3a}
\DoxyCodeLine{1. Add xlinalg.QR}
\DoxyCodeLine{2. enhance hosvd.}
\DoxyCodeLine{3. Fix bug in cytnx.linalg.Abs truncate the floating point part. }
\DoxyCodeLine{4. Add example for HOTRG}
\DoxyCodeLine{5. Add example for iDMRG}
\DoxyCodeLine{6. Add CyTensor.truncate/truncate.}
\DoxyCodeLine{7. Add linalg::Sum. }
\DoxyCodeLine{8. Complete set\_elem for sparse CyTensor dispatch in binding.}
\DoxyCodeLine{9. [Important] Change Inv/Inv\_ to InvM/InvM\_ for matrix inverse. }
\DoxyCodeLine{10. [Important] Add Inv/Inv\_ for elementwise inverse with clip. }
\DoxyCodeLine{11. [Enhance] Add str\_strip for removing "{} "{}, "{}\(\backslash\)t"{}, "{}\(\backslash\)r"{} at the end.}
\DoxyCodeLine{12. [Enhance] Accessor::() allow negative input.}
\DoxyCodeLine{13. Add GPU Pow/Pow\_}
\DoxyCodeLine{14. Add random.uniform()}
\DoxyCodeLine{15. Fix bug in diagonal CyTensor reshape/reshape\_ cause mismatch.}
\DoxyCodeLine{16. Add a is\_diag option for convert Tensor to CyTensor. }
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.2a-\/build1}
\DoxyCodeLine{1. example/iTEBD, please modify the argument rowrank-\/>Rowrank if you encounter error in running them.}
\DoxyCodeLine{2. Fix bug in cytnx.linalg.Abs truncate floating point part. -\/-\/-\/> v0.5.2a-\/build1}
\DoxyCodeLine{3. Fix bug in mkl blas package import bug with numpy.        -\/-\/-\/> v0.5.2a-\/build1}
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.2a}
\DoxyCodeLine{1. add Trace and Trace\_ for CyTensor.}
\DoxyCodeLine{2. fix bug in Network.Launch does not return the output CyTensor}
\DoxyCodeLine{3. Add Network.PrintNet, and ostream support.}
\DoxyCodeLine{4. Add Network.Diagram() for plot the tensor network diagram (python only)    }
\DoxyCodeLine{5. Add support for floating type Vectordot on GPU. }
\DoxyCodeLine{6. Fix bug in to from Anytype to ComplexFloat. }
\DoxyCodeLine{7. Add QR for CPU.}
\DoxyCodeLine{8. Add identity() and it's alias function eye(). }
\DoxyCodeLine{9. Add physics namespace/submodule}
\DoxyCodeLine{10. Add physics::spin() for generating Spin-\/S representation. }
\DoxyCodeLine{11. Add physics::pauli() for pauli matrix.}
\DoxyCodeLine{12. Add ExpM() for generic matrix (CPU only)}
\DoxyCodeLine{13. Fix bug in python slice, and reverse range slice.}
\DoxyCodeLine{14. Enhance optional Kron padding scheme}
\DoxyCodeLine{15. Fix bug in CyTensor contract/Contract(A,B) for tensors with no common label}
\DoxyCodeLine{16. Enhance error message in Network}
\DoxyCodeLine{17. Add Min(), Max() (CPU only)}
\DoxyCodeLine{18. Fix bug in Abs. }
\DoxyCodeLine{19. Fix bug in Div i32td.    }
\DoxyCodeLine{20. [Feature] Add optimal contraction order calculation in Network}
\DoxyCodeLine{21. Fix SparseCyTensor contiguous address wrong calculation. }
\DoxyCodeLine{22. Support at() directly from SparseCyTensor.}
\DoxyCodeLine{23. Add Transpose, Dagger to CyTensor. For tagged CyTensor, Transpose/Dagger will reverse the direction of all bonds.}
\DoxyCodeLine{24. Add xlinalg.Svd, xlinalg.Svd\_truncate support for tagged CyTensor.}
\DoxyCodeLine{25. Fix redundant print in optimal contraction order}
\DoxyCodeLine{26. Add CyTensor.tag() for DenseCyTensor (regular type) directly convert to CyTensor with direction (tagged type)}
\DoxyCodeLine{27. Add SparseCyTensor.at (currently only floating point type) }
\DoxyCodeLine{28. SparseCyTensor.ele\_exists. }
\DoxyCodeLine{29. SparseCyTensor.Transpose, Conj. }
\DoxyCodeLine{30. Symmetry.reverse\_rule, Bond.calc\_reverse\_qnums}
\DoxyCodeLine{31. Fix Tensor.numpy from GPU bug.}
\DoxyCodeLine{32. Fix Tensor.setitem/getitem pybind bug.}
\DoxyCodeLine{33. SparseCyTensor.get\_elem/set\_elem (currently floating type only (complex)) }
\DoxyCodeLine{34. Add xlinalg::ExpH, xlinalg::ExpM, xlinalg::Trace (ovld of CyTensor.Trace)}
\DoxyCodeLine{35. support Mul/Div operation on SparseCyTensor }
\DoxyCodeLine{36. Add Tensor.flatten();}
\DoxyCodeLine{37. Add Network.Savefile. Network.PutCyTensors}
\DoxyCodeLine{38. [Feature] Tensor can now use unify operator[] to get and set elements as python API}
\DoxyCodeLine{39. fix ambiguous error message in Tensor arithmetic.}
\DoxyCodeLine{40. fix bug in xlinalg::Svd}
\DoxyCodeLine{41. fix bug in physics::pauli}
\DoxyCodeLine{42. fix bug in CyTensor.set\_label checking element.}
\DoxyCodeLine{43. Add xlinalg::Hosvd (currently CyTensor only)}
\DoxyCodeLine{44. change argument of init CyTensor rowrank-\/>Rowrank}
\DoxyCodeLine{45. Add PESS example }
\DoxyCodeLine{46. Add support for Norm to generic rank-\/N Tensor}
\DoxyCodeLine{47. Add @ operator in python API for shorthand of linalg::Dot}
\DoxyCodeLine{48. Add DMRG example}
\DoxyCodeLine{49. C++ API can now have accessor.size() < rank()}
\DoxyCodeLine{50. Remove redundant output of Inv.}
\DoxyCodeLine{51. Add Pow, Pow\_ for CyTensor.}
\DoxyCodeLine{52. Add Symmetry.Save/Load}
\DoxyCodeLine{53. Symmetry/Tensor/Storage/Bond/CyTensor Save/Load re-\/invented for more simple usage}
\DoxyCodeLine{}
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.1a}
\DoxyCodeLine{1. add Norm() for CPU and GPU, add to call by Tn}
\DoxyCodeLine{2. add Dot() for CPU and GPU, with unify API for Vec-\/Vec/Mat-\/Vec/Mat-\/Mat/Ten-\/Vec product.}
\DoxyCodeLine{3. add Tensor.rank() }
\DoxyCodeLine{4. [Feature] support Tensor <-\/> numpy.ndarray}
\DoxyCodeLine{5. add random::Make\_uniform()}
\DoxyCodeLine{6. Fix bug in Svd\_truncate that will change the underlying block for contiguous CyTensor. }
\DoxyCodeLine{7. Fix bug in Tensor-\/>numpy if the underlying Tensor is non-\/contiguous. }
\DoxyCodeLine{8. Add Eig.}
\DoxyCodeLine{9. Add real() imag() for Tensor. }
\DoxyCodeLine{10. Enhance python API, Storage \& Tensor are now iterable.}
\DoxyCodeLine{11. Fix buf in Conj and Conj\_, for both C++ and python}
\DoxyCodeLine{12. Fix bug python inplace call return ID Conj\_, Inv\_, Exp\_}
\DoxyCodeLine{13. Add Conj, Conj\_ for CyTensor}
\DoxyCodeLine{14. Fix non-\/inplace Arithmetic for non-\/contiguous tensor. }
\DoxyCodeLine{15. Add [trial version] Trace. }
\DoxyCodeLine{16. Add Pow, Pow\_ for cpu. }
\DoxyCodeLine{17. Add Abs, Abs\_ for cpu.}
\DoxyCodeLine{}
\DoxyCodeLine{v0.5.0a}
\DoxyCodeLine{1. Add .imag() .real() for Storage. }
\DoxyCodeLine{2. Add xlinalg under cytnx\_extension for algebra on CyTensor}
\DoxyCodeLine{3. Add xlinalg::Svd()  }
\DoxyCodeLine{4. Change linalg::Eigh() to match numpy }
\DoxyCodeLine{5. fix Diag uninitialize elemets bug}
\DoxyCodeLine{6. add linalg::ExpH()}
\DoxyCodeLine{7. add random::Make\_normal()}
\DoxyCodeLine{8. add iTEBD example for both C++ and python @ example/iTEBD}
\DoxyCodeLine{}
\DoxyCodeLine{v0.4}
\DoxyCodeLine{1. remove Otimes, add Kron and Outer }
\DoxyCodeLine{2. Add Storage append, capacity, pre-\/alloc 32x address}
\DoxyCodeLine{3. Tensor can now allow redundant dimension (e.g. shape = (1,1,1,1,1...) }
\DoxyCodeLine{4. Add Storage.from\_vector, directly convert the C++ vector to Storage}
\DoxyCodeLine{5. Add more intruisive way to get slices for Tensor in C++, using operator[]}
\DoxyCodeLine{6. Add Tensor.append for rank-\/1 Tensor    }
\DoxyCodeLine{7. Add Exp() Expf() Exp\(\backslash\)\_() Expf\(\backslash\)\_()}
\DoxyCodeLine{8. Change UniTensor to CyTensor }
\DoxyCodeLine{9. Guarded CyTensor, Bond, Symmetry and Network class with cytnx\_extension namespace (cytnx\_extension submodule in python).  }

\end{DoxyCodeInclude}
 